{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61ef3572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any,TypedDict\n",
    "import uuid \n",
    "\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.constants import START\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import Command,interrupt\n",
    "\n",
    "load_dotenv()\n",
    "# https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/add-human-in-the-loop/#pause-using-interrupt\n",
    "# https://langchain-ai.github.io/langgraph/how-tos/memory/add-memory/#patterns\n",
    "# https://langchain-ai.github.io/langgraph/tutorials/get-started/4-human-in-the-loop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63a0ed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Interrupt(value={'text_to_revise': 'Hello, world!'}, resumable=True, ns=['human_node:f14cc7f6-727b-16d5-0614-86fd0170d4b1'])]\n",
      "{'some_text': 'Edited text'}\n"
     ]
    }
   ],
   "source": [
    "class State(TypedDict):\n",
    "    some_text: str\n",
    "\n",
    "def human_node(state: State):\n",
    "    value = interrupt({\n",
    "        'text_to_revise': state['some_text']\n",
    "    })\n",
    "    return {'some_text': value}\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"human_node\", human_node)\n",
    "graph_builder.add_edge(START, \"human_node\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": 'thread_1'}}\n",
    "\n",
    "result = graph.invoke({\"some_text\": \"Hello, world!\"}, config=config)\n",
    "\n",
    "print(result['__interrupt__'])\n",
    "    \n",
    "print(graph.invoke(Command(resume=\"Edited text\"), config=config)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da24f056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'some_text': 'Hello, world!',\n",
       " '__interrupt__': [Interrupt(value={'text_to_revise': 'Hello, world!'}, resumable=True, ns=['human_node:e6721fd7-e076-5eb3-ba14-cfe4ca85ca0b'])]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9104705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'llm_output': 'This is the generated output.', '__interrupt__': [Interrupt(value={'question': 'Do you approve the following output?', 'llm_output': 'This is the generated output.'}, resumable=True, ns=['human_approval:1b143ea6-98f3-1554-7fa6-5c857e41597a'])]}\n",
      "✅ Approved path taken.\n",
      "{'llm_output': 'This is the generated output.', 'decision': 'approved'}\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal, TypedDict\n",
    "import uuid\n",
    "\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Define the shared graph state\n",
    "class State(TypedDict):\n",
    "    llm_output: str\n",
    "    decision: str\n",
    "\n",
    "# Simulate an LLM output node\n",
    "def generate_llm_output(state: State) -> State:\n",
    "    return {\"llm_output\": \"This is the generated output.\"}\n",
    "\n",
    "# Human approval node\n",
    "def human_approval(state: State) -> Command[Literal[\"approved_path\", \"rejected_path\"]]:\n",
    "    decision = interrupt({\n",
    "        \"question\": \"Do you approve the following output?\",\n",
    "        \"llm_output\": state[\"llm_output\"]\n",
    "    })\n",
    "\n",
    "    if decision == \"approve\":\n",
    "        return Command(goto=\"approved_path\", update={\"decision\": \"approved\"})\n",
    "    else:\n",
    "        return Command(goto=\"rejected_path\", update={\"decision\": \"rejected\"})\n",
    "\n",
    "# Next steps after approval\n",
    "def approved_node(state: State) -> State:\n",
    "    print(\"✅ Approved path taken.\")\n",
    "    return state\n",
    "\n",
    "# Alternative path after rejection\n",
    "def rejected_node(state: State) -> State:\n",
    "    print(\"❌ Rejected path taken.\")\n",
    "    return state\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"generate_llm_output\", generate_llm_output)\n",
    "builder.add_node(\"human_approval\", human_approval)\n",
    "builder.add_node(\"approved_path\", approved_node)\n",
    "builder.add_node(\"rejected_path\", rejected_node)\n",
    "\n",
    "builder.set_entry_point(\"generate_llm_output\")\n",
    "builder.add_edge(\"generate_llm_output\", \"human_approval\")\n",
    "builder.add_edge(\"approved_path\", END)\n",
    "builder.add_edge(\"rejected_path\", END)\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Run until interrupt\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "result = graph.invoke({}, config=config)\n",
    "print(result)\n",
    "# Output:\n",
    "# Interrupt(value={'question': 'Do you approve the following output?', 'llm_output': 'This is the generated output.'}, ...)\n",
    "\n",
    "# Simulate resuming with human input\n",
    "# To test rejection, replace resume=\"approve\" with resume=\"reject\"\n",
    "final_result = graph.invoke(Command(resume=\"approve\"), config=config)\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f568a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Interrupt(value={'task': 'Please review and edit the generated summary if necessary.', 'generated_summary': 'The cat sat on the mat and looked at the stars.'}, resumable=True, ns=['human_review_edit:bd593371-0567-ef6c-ae37-c11e96b98dd0'])]\n",
      "✅ Using edited summary: The cat lay on the rug, gazing peacefully at the night sky.\n",
      "{'summary': 'The cat lay on the rug, gazing peacefully at the night sky.'}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "import uuid\n",
    "\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Define the graph state\n",
    "class State(TypedDict):\n",
    "    summary: str\n",
    "\n",
    "# Simulate an LLM summary generation\n",
    "def generate_summary(state: State) -> State:\n",
    "    return {\n",
    "        \"summary\": \"The cat sat on the mat and looked at the stars.\"\n",
    "    }\n",
    "\n",
    "# Human editing node\n",
    "def human_review_edit(state: State) -> State:\n",
    "    result = interrupt({\n",
    "        \"task\": \"Please review and edit the generated summary if necessary.\",\n",
    "        \"generated_summary\": state[\"summary\"]\n",
    "    })\n",
    "    return {\n",
    "        \"summary\": result[\"edited_summary\"]\n",
    "    }\n",
    "\n",
    "# Simulate downstream use of the edited summary\n",
    "def downstream_use(state: State) -> State:\n",
    "    print(f\"✅ Using edited summary: {state['summary']}\")\n",
    "    return state\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"generate_summary\", generate_summary)\n",
    "builder.add_node(\"human_review_edit\", human_review_edit)\n",
    "builder.add_node(\"downstream_use\", downstream_use)\n",
    "\n",
    "builder.set_entry_point(\"generate_summary\")\n",
    "builder.add_edge(\"generate_summary\", \"human_review_edit\")\n",
    "builder.add_edge(\"human_review_edit\", \"downstream_use\")\n",
    "builder.add_edge(\"downstream_use\", END)\n",
    "\n",
    "# Set up in-memory checkpointing for interrupt support\n",
    "checkpointer = MemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Invoke the graph until it hits the interrupt\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "result = graph.invoke({}, config=config)\n",
    "\n",
    "# Output interrupt payload\n",
    "print(result[\"__interrupt__\"])\n",
    "# Example output:\n",
    "# Interrupt(\n",
    "#   value={\n",
    "#     'task': 'Please review and edit the generated summary if necessary.',\n",
    "#     'generated_summary': 'The cat sat on the mat and looked at the stars.'\n",
    "#   },\n",
    "#   resumable=True,\n",
    "#   ...\n",
    "# )\n",
    "\n",
    "# Resume the graph with human-edited input\n",
    "edited_summary = \"The cat lay on the rug, gazing peacefully at the night sky.\"\n",
    "resumed_result = graph.invoke(\n",
    "    Command(resume={\"edited_summary\": edited_summary}),\n",
    "    config=config\n",
    ")\n",
    "print(resumed_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "506b3c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=1,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b328d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x12b0353a0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.tools import tool\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "@tool\n",
    "def human_assistance(query: str) -> str:\n",
    "    \"\"\"Request assistance from a human.\"\"\"\n",
    "    human_response = interrupt({\"query\": query})\n",
    "    return human_response[\"data\"]\n",
    "\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool, human_assistance]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    message = llm_with_tools.invoke(state[\"messages\"])\n",
    "    # Because we will be interrupting during tool execution,\n",
    "    # we disable parallel tool calling to avoid repeating any\n",
    "    # tool invocations when we resume.\n",
    "    assert len(message.tool_calls) <= 1\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "769a3200",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bb67ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bab0fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I need some expert guidance for building an AI agent. Could you request assistance for me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  human_assistance (call_0_5198bbaa-a906-4e25-af01-55a1a0aeb90c)\n",
      " Call ID: call_0_5198bbaa-a906-4e25-af01-55a1a0aeb90c\n",
      "  Args:\n",
      "    query: I need expert guidance for building an AI agent.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"I need some expert guidance for building an AI agent. Could you request assistance for me?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "events = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "252c01b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tools',)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "snapshot.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37a2f4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  human_assistance (call_0_bb59d6ce-e99c-4f69-921f-f37ad66d716f)\n",
      " Call ID: call_0_bb59d6ce-e99c-4f69-921f-f37ad66d716f\n",
      "  Args:\n",
      "    query: I need expert guidance for building an AI agent. Could you connect me with a specialist who can assist with the development process, including design, implementation, and deployment?\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: human_assistance\n",
      "\n",
      "We, the experts are here to help! We'd recommend you check out LangGraph to build your agent. It's much more reliable and extensible than simple autonomous agents.can you search the LangGraph in web an give a detail answer\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (call_0_23482ccc-24b1-4ff6-acf3-329fd12e0cb8)\n",
      " Call ID: call_0_23482ccc-24b1-4ff6-acf3-329fd12e0cb8\n",
      "  Args:\n",
      "    query: LangGraph\n",
      "    search_depth: advanced\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{\"query\": \"LangGraph\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.ibm.com/think/topics/langgraph\", \"title\": \"What is LangGraph? - IBM\", \"content\": \"LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. It provides a set of tools and libraries that enable users to create, run and optimize large language models (LLMs) in a scalable and efficient manner. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. [...] Agent systems: LangGraph provides a framework for building agent-based systems, which can be used in applications such as robotics, autonomous vehicles or video games.\\n\\nLLM applications: By using LangGraph’s capabilities, developers can build more sophisticated AI models that learn and improve over time. Norwegian Cruise Line uses LangGraph to compile, construct and refine guest-facing AI solutions. This capability allows for improved and personalized guest experiences. [...] By using a graph-based architecture, LangGraph enables users to scale artificial intelligence workflows without slowing down or sacrificing efficiency. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback. In the world of LLMs, this process is referred to as reflection.\", \"score\": 0.9353998, \"raw_content\": null}, {\"title\": \"What is LangGraph? - GeeksforGeeks\", \"url\": \"https://www.geeksforgeeks.org/machine-learning/what-is-langgraph/\", \"content\": \"LangGraph is a Python library that helps you build applications like chatbots or AI agents by organizing their logic step-by-step using state machine model. This means you can define how the application should behave at each step such as classifying input, generating a response or calling a tool and the application remembers its progress by storing information in a shared state.\", \"score\": 0.9301101, \"raw_content\": null}], \"response_time\": 1.17}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here’s a detailed overview of **LangGraph**, a powerful framework for building AI agents:\n",
      "\n",
      "### **What is LangGraph?**\n",
      "LangGraph is an open-source AI agent framework developed by LangChain. It is designed to help developers build, deploy, and manage complex generative AI workflows. The framework leverages **graph-based architectures** to model and optimize the relationships between different components of an AI agent system.\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Features of LangGraph**\n",
      "1. **Graph-Based Architecture**:\n",
      "   - LangGraph uses nodes and edges to represent workflows, enabling scalable and efficient management of AI tasks.\n",
      "   - This architecture allows for **dynamic decision-making** and reflection, where AI agents can analyze past actions and feedback to improve performance.\n",
      "\n",
      "2. **State Machine Model**:\n",
      "   - It organizes the logic of AI applications (e.g., chatbots, autonomous agents) step-by-step, storing progress in a shared state.\n",
      "   - Developers can define how the application behaves at each stage, such as classifying input, generating responses, or calling external tools.\n",
      "\n",
      "3. **Scalability**:\n",
      "   - LangGraph is optimized for large-scale AI workflows, ensuring efficiency even as complexity grows.\n",
      "   - It supports **parallel processing** and distributed computing for high-performance applications.\n",
      "\n",
      "4. **Integration with LLMs**:\n",
      "   - The framework is compatible with large language models (LLMs), enabling developers to build sophisticated AI models that learn and adapt over time.\n",
      "\n",
      "5. **Use Cases**:\n",
      "   - **Agent Systems**: Robotics, autonomous vehicles, and video games.\n",
      "   - **LLM Applications**: Personalized customer experiences (e.g., Norwegian Cruise Line uses it for guest-facing AI solutions).\n",
      "   - **Chatbots and AI Assistants**: Step-by-step logic for conversational agents.\n",
      "\n",
      "---\n",
      "\n",
      "### **How LangGraph Works**\n",
      "- Developers define workflows as graphs, where nodes represent tasks (e.g., data processing, API calls) and edges define transitions between tasks.\n",
      "- The framework manages the state of the workflow, ensuring seamless execution and error handling.\n",
      "- It supports **reflection**, a process where AI agents evaluate their actions to improve future performance.\n",
      "\n",
      "---\n",
      "\n",
      "### **Getting Started with LangGraph**\n",
      "1. **Installation**:\n",
      "   - LangGraph is available as a Python library. You can install it via pip:\n",
      "     ```bash\n",
      "     pip install langgraph\n",
      "     ```\n",
      "\n",
      "2. **Documentation**:\n",
      "   - Explore the official documentation for tutorials and examples:\n",
      "     - [IBM's LangGraph Overview](https://www.ibm.com/think/topics/langgraph)\n",
      "     - [GeeksforGeeks Introduction](https://www.geeksforgeeks.org/machine-learning/what-is-langgraph/)\n",
      "\n",
      "3. **Community and Support**:\n",
      "   - Join forums or GitHub repositories related to LangGraph for community-driven support and updates.\n",
      "\n",
      "---\n",
      "\n",
      "### **Why Choose LangGraph?**\n",
      "- **Flexibility**: Adaptable to various AI applications, from simple chatbots to complex autonomous systems.\n",
      "- **Efficiency**: Optimized for performance and scalability.\n",
      "- **Open-Source**: Free to use and modify, with active community contributions.\n",
      "\n",
      "If you’re looking for a robust framework to build AI agents, LangGraph is a strong candidate. Would you like help with a specific implementation or use case?\n"
     ]
    }
   ],
   "source": [
    "human_response = (\n",
    "    \"We, the experts are here to help! We'd recommend you check out LangGraph to build your agent.\"\n",
    "    \" It's much more reliable and extensible than simple autonomous agents.\"\n",
    "    \"can you search the LangGraph in web an give a detail answer\"\n",
    ")\n",
    "\n",
    "human_command = Command(resume={\"data\": human_response})\n",
    "\n",
    "events = graph.stream(human_command, config, stream_mode=\"values\")\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d81d1db3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m event:\n\u001b[0;32m----> 5\u001b[0m         event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpretty_print()\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "user_input='introduce the deepseek model'\n",
    "events = graph.invoke(    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},config,)\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13387be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=1,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30d9ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.types import interrupt\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# An example of a sensitive tool that requires human review / approval\n",
    "def book_hotel(hotel_name: str):\n",
    "    \"\"\"Book a hotel\"\"\"\n",
    "    response = interrupt(  \n",
    "        f\"Trying to call `book_hotel` with args {{'hotel_name': {hotel_name}}}. \"\n",
    "        \"Please approve or suggest edits.\"\n",
    "    )\n",
    "    if response[\"type\"] == \"accept\":\n",
    "        pass\n",
    "    elif response[\"type\"] == \"edit\":\n",
    "        hotel_name = response[\"args\"][\"hotel_name\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown response type: {response['type']}\")\n",
    "    return f\"Successfully booked a stay at {hotel_name}.\"\n",
    "\n",
    "checkpointer = InMemorySaver() \n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[book_hotel],\n",
    "    checkpointer=checkpointer, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbb4dbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_0_3554361c-bc12-405f-9c5a-978ccad48cf7', 'function': {'arguments': '{\"hotel_name\":\"McKittrick hotel\"}', 'name': 'book_hotel'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 111, 'total_tokens': 146, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 111}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0623_fp8_kvcache', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--1a7560eb-a56a-4d12-a075-a7e3476eb846-0', tool_calls=[{'name': 'book_hotel', 'args': {'hotel_name': 'McKittrick hotel'}, 'id': 'call_0_3554361c-bc12-405f-9c5a-978ccad48cf7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 111, 'output_tokens': 35, 'total_tokens': 146, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})]}}\n",
      "\n",
      "\n",
      "{'__interrupt__': (Interrupt(value=\"Trying to call `book_hotel` with args {'hotel_name': McKittrick hotel}. Please approve or suggest edits.\", resumable=True, ns=['tools:0228b182-471b-5660-08fc-e9b890394ea4']),)}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "   \"configurable\": {\n",
    "      \"thread_id\": \"1\"\n",
    "   }\n",
    "}\n",
    "\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"book a stay at McKittrick hotel\"}]},\n",
    "    config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "839ae67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tools': {'messages': [ToolMessage(content='Successfully booked a stay at McKittrick hotel.', name='book_hotel', id='cba6be42-894e-4cd0-b9c9-a4b70f9b813e', tool_call_id='call_0_3554361c-bc12-405f-9c5a-978ccad48cf7')]}}\n",
      "\n",
      "\n",
      "{'agent': {'messages': [AIMessage(content=\"Your stay at the McKittrick hotel has been successfully booked! Let me know if there's anything else you'd like to arrange.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 153, 'total_tokens': 180, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 64}, 'prompt_cache_hit_tokens': 64, 'prompt_cache_miss_tokens': 89}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_8802369eaa_prod0623_fp8_kvcache', 'finish_reason': 'stop', 'logprobs': None}, id='run--0e5ba368-4710-40fe-bcfe-f4ccc180f020-0', usage_metadata={'input_tokens': 153, 'output_tokens': 27, 'total_tokens': 180, 'input_token_details': {'cache_read': 64}, 'output_token_details': {}})]}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langgraph.types import Command\n",
    "\n",
    "for chunk in agent.stream(\n",
    "    Command(resume={\"type\": \"accept\"}),  \n",
    "    # Command(resume={\"type\": \"edit\", \"args\": {\"hotel_name\": \"McKittrick Hotel\"}}),\n",
    "    config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b1620ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Interrupt(value='Please enter your age (must be a non-negative integer).', resumable=True, ns=['get_valid_age:8bff879a-ed1e-e0bd-8c0e-5ff98d0ae8f6'])]\n",
      "[Interrupt(value=\"'not a number' is not valid. Please enter a non-negative integer for age.\", resumable=True, ns=['get_valid_age:8bff879a-ed1e-e0bd-8c0e-5ff98d0ae8f6'])]\n",
      "[Interrupt(value=\"'-10' is not valid. Please enter a non-negative integer for age.\", resumable=True, ns=['get_valid_age:8bff879a-ed1e-e0bd-8c0e-5ff98d0ae8f6'])]\n",
      "✅ Human is 25 years old.\n",
      "{'age': 25}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "import uuid\n",
    "\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Define graph state\n",
    "class State(TypedDict):\n",
    "    age: int\n",
    "\n",
    "# Node that asks for human input and validates it\n",
    "def get_valid_age(state: State) -> State:\n",
    "    prompt = \"Please enter your age (must be a non-negative integer).\"\n",
    "\n",
    "    while True:\n",
    "        user_input = interrupt(prompt)\n",
    "\n",
    "        # Validate the input\n",
    "        try:\n",
    "            age = int(user_input)\n",
    "            if age < 0:\n",
    "                raise ValueError(\"Age must be non-negative.\")\n",
    "            break  # Valid input received\n",
    "        except (ValueError, TypeError):\n",
    "            prompt = f\"'{user_input}' is not valid. Please enter a non-negative integer for age.\"\n",
    "\n",
    "    return {\"age\": age}\n",
    "\n",
    "# Node that uses the valid input\n",
    "def report_age(state: State) -> State:\n",
    "    print(f\"✅ Human is {state['age']} years old.\")\n",
    "    return state\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"get_valid_age\", get_valid_age)\n",
    "builder.add_node(\"report_age\", report_age)\n",
    "\n",
    "builder.set_entry_point(\"get_valid_age\")\n",
    "builder.add_edge(\"get_valid_age\", \"report_age\")\n",
    "builder.add_edge(\"report_age\", END)\n",
    "\n",
    "# Create the graph with a memory checkpointer\n",
    "checkpointer = MemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Run the graph until the first interrupt\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "result = graph.invoke({}, config=config)\n",
    "print(result[\"__interrupt__\"])  # First prompt: \"Please enter your age...\"\n",
    "\n",
    "# Simulate an invalid input (e.g., string instead of integer)\n",
    "result = graph.invoke(Command(resume=\"not a number\"), config=config)\n",
    "print(result[\"__interrupt__\"])  # Follow-up prompt with validation message\n",
    "\n",
    "# Simulate a second invalid input (e.g., negative number)\n",
    "result = graph.invoke(Command(resume=\"-10\"), config=config)\n",
    "print(result[\"__interrupt__\"])  # Another retry\n",
    "\n",
    "# Provide valid input\n",
    "final_result = graph.invoke(Command(resume=\"25\"), config=config)\n",
    "print(final_result)  # Should include the valid age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4791a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
