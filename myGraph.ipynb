{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f633dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from google import genai\n",
    "\n",
    "# pass your API key here\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\") or getpass(\n",
    "    \"Enter Google API Key: \"\n",
    ")\n",
    "# initialize our client\n",
    "client = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a42bbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Gemini refers to a few different things, but most commonly it refers to **Google's large language model.** Here's a breakdown:\n",
       "\n",
       "**1. Gemini (The Google AI Model):**\n",
       "\n",
       "*   **What it is:** Gemini is a multimodal AI model developed by Google AI. It's designed to be Google's most capable and general AI model, surpassing previous models like LaMDA and PaLM.\n",
       "*   **Capabilities:** It can understand and generate text, images, audio, video, and code. This \"multimodal\" nature means it can process and understand information from various sources simultaneously. It's designed to be highly adaptable and perform well on a wide range of tasks.\n",
       "*   **Versions:** There are different versions of Gemini optimized for various use cases:\n",
       "    *   **Gemini Ultra:** The most powerful version, designed for highly complex tasks.\n",
       "    *   **Gemini Pro:** A more balanced version, suitable for a wide range of applications, including powering Google's Bard (now Gemini) chatbot.\n",
       "    *   **Gemini Nano:** A lightweight version designed to run on devices like smartphones and other edge devices.\n",
       "*   **Use Cases:** Gemini is being integrated into various Google products and services, including:\n",
       "    *   **Search:** Improving search results and providing more comprehensive answers.\n",
       "    *   **Gmail & Workspace:** Helping with email composition, summarizing documents, and generating ideas.\n",
       "    *   **Bard/Gemini Chatbot:** Providing more natural and helpful conversational AI.\n",
       "    *   **Coding:** Assisting developers with code generation, debugging, and understanding code.\n",
       "    *   **Creative Content:** Helping with writing, image generation, and other creative tasks.\n",
       "\n",
       "**2. Gemini (The Zodiac Sign):**\n",
       "\n",
       "*   In astrology, Gemini is the third sign of the zodiac, associated with people born between approximately May 21 and June 20. It's an air sign often associated with communication, curiosity, and adaptability.\n",
       "\n",
       "**3. Project Gemini (NASA):**\n",
       "\n",
       "*   In the 1960s, Project Gemini was a NASA human spaceflight program that served as a bridge between Project Mercury (the first US human spaceflight program) and Project Apollo (the program that landed humans on the Moon). Gemini developed techniques for rendezvous, docking, and spacewalking that were crucial for the success of Apollo.\n",
       "\n",
       "**In summary, when someone mentions \"Gemini\" today, they are most likely referring to the Google AI model.**  To avoid confusion, it's helpful to consider the context in which the word is used.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "model_id = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=model_id, contents='What is Gemini?'\n",
    ")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28cc352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai.types import Tool, GoogleSearch\n",
    "\n",
    "search_tool = Tool(google_search=GoogleSearch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef846338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentConfig(http_options=None, system_instruction='You are a helpful assistant that provides up to date information to help the user in their research.', temperature=0.0, top_p=None, top_k=None, candidate_count=1, max_output_tokens=None, stop_sequences=None, response_logprobs=None, logprobs=None, presence_penalty=None, frequency_penalty=None, seed=None, response_mime_type=None, response_schema=None, routing_config=None, model_selection_config=None, safety_settings=None, tools=[Tool(retrieval=None, google_search=GoogleSearch(), google_search_retrieval=None, enterprise_web_search=None, google_maps=None, code_execution=None, function_declarations=None)], tool_config=None, labels=None, cached_content=None, response_modalities=['TEXT'], media_resolution=None, speech_config=None, audio_timestamp=None, automatic_function_calling=None, thinking_config=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.genai.types import GenerateContentConfig\n",
    "model_id = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "config = GenerateContentConfig(\n",
    "    system_instruction=(\n",
    "        \"You are a helpful assistant that provides up to date information \"\n",
    "        \"to help the user in their research.\"\n",
    "    ),\n",
    "    tools=[search_tool],\n",
    "    response_modalities=[\"TEXT\"],\n",
    "    temperature=0.0,  # likely default\n",
    "    candidate_count=1,  # likely default\n",
    "\n",
    ")\n",
    "config\n",
    "response = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    contents=\"Tell me the latest news in AI\",\n",
    "    config=config\n",
    ")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da03f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    contents=\"Tell me the latest news in AI\",\n",
    "    config=config\n",
    ")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a33b00b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.0-flash-exp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     contents\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me the latest news in AI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     config\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle_search\u001b[39m\u001b[38;5;124m\"\u001b[39m: {}}],\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      7\u001b[0m         },\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m      9\u001b[0m Markdown(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/google/genai/models.py:5202\u001b[0m, in \u001b[0;36mModels.generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5200\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   5201\u001b[0m   i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 5202\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_content(\n\u001b[1;32m   5203\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel, contents\u001b[38;5;241m=\u001b[39mcontents, config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[1;32m   5204\u001b[0m   )\n\u001b[1;32m   5205\u001b[0m   logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is done.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   5206\u001b[0m   remaining_remote_calls_afc \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/google/genai/models.py:4178\u001b[0m, in \u001b[0;36mModels._generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4175\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mconvert_to_dict(request_dict)\n\u001b[1;32m   4176\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[0;32m-> 4178\u001b[0m response_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m   4179\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m, path, request_dict, http_options\n\u001b[1;32m   4180\u001b[0m )\n\u001b[1;32m   4182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mvertexai:\n\u001b[1;32m   4183\u001b[0m   response_dict \u001b[38;5;241m=\u001b[39m _GenerateContentResponse_from_vertex(\n\u001b[1;32m   4184\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client, response_dict\n\u001b[1;32m   4185\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/google/genai/_api_client.py:755\u001b[0m, in \u001b[0;36mBaseApiClient.request\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    747\u001b[0m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    750\u001b[0m     http_options: Optional[HttpOptionsOrDict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    751\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[BaseResponse, Any]:\n\u001b[1;32m    752\u001b[0m   http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[1;32m    753\u001b[0m       http_method, path, request_dict, http_options\n\u001b[1;32m    754\u001b[0m   )\n\u001b[0;32m--> 755\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(http_request, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    756\u001b[0m   json_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson\n\u001b[1;32m    757\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/google/genai/_api_client.py:677\u001b[0m, in \u001b[0;36mBaseApiClient._request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    673\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[1;32m    674\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[1;32m    675\u001b[0m   )\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 677\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpx_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    678\u001b[0m       method\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    679\u001b[0m       url\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    680\u001b[0m       headers\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    681\u001b[0m       content\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    682\u001b[0m       timeout\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    683\u001b[0m   )\n\u001b[1;32m    684\u001b[0m   errors\u001b[38;5;241m.\u001b[39mAPIError\u001b[38;5;241m.\u001b[39mraise_for_response(response)\n\u001b[1;32m    685\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[1;32m    686\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[1;32m    687\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:825\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    810\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    812\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    813\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    814\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    823\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    824\u001b[0m )\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(request, auth\u001b[38;5;241m=\u001b[39mauth, follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m    915\u001b[0m     request,\n\u001b[1;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    919\u001b[0m )\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m    943\u001b[0m         request,\n\u001b[1;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection.py:99\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection.py:76\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_lock:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect(request)\n\u001b[1;32m     78\u001b[0m         ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m         http2_negotiated \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     80\u001b[0m             ssl_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     81\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m ssl_object\u001b[38;5;241m.\u001b[39mselected_alpn_protocol() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection.py:122\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    114\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhost\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_origin\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_origin\u001b[38;5;241m.\u001b[39mport,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocket_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket_options,\n\u001b[1;32m    120\u001b[0m     }\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 122\u001b[0m         stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_backend\u001b[38;5;241m.\u001b[39mconnect_tcp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    123\u001b[0m         trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_backends/sync.py:206\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    200\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    201\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    203\u001b[0m }\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m--> 206\u001b[0m     sock \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    207\u001b[0m         address,\n\u001b[1;32m    208\u001b[0m         timeout,\n\u001b[1;32m    209\u001b[0m         source_address\u001b[38;5;241m=\u001b[39msource_address,\n\u001b[1;32m    210\u001b[0m     )\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m socket_options:\n\u001b[1;32m    212\u001b[0m         sock\u001b[38;5;241m.\u001b[39msetsockopt(\u001b[38;5;241m*\u001b[39moption)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:836\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m    835\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m--> 836\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m    838\u001b[0m exceptions\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    contents=\"Tell me the latest news in AI\",\n",
    "    config={\n",
    "        \"tools\": [{\"google_search\": {}}],\n",
    "        \"temperature\": 0,\n",
    "        },\n",
    "    )\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea59c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd370cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_google_community import GoogleSearchAPIWrapper\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"google_search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=search.run,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1afd9fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tool\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObama\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms first name?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/tools/base.py:774\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[1;32m    773\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(error_to_raise)\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[1;32m    775\u001b[0m output \u001b[38;5;241m=\u001b[39m _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, status)\n\u001b[1;32m    776\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(output, color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/tools/base.py:743\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config_param \u001b[38;5;241m:=\u001b[39m _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run):\n\u001b[1;32m    742\u001b[0m         tool_kwargs \u001b[38;5;241m=\u001b[39m tool_kwargs \u001b[38;5;241m|\u001b[39m {config_param: config}\n\u001b[0;32m--> 743\u001b[0m     response \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run, \u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_and_artifact\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/tools/simple.py:105\u001b[0m, in \u001b[0;36mTool._run\u001b[0;34m(self, config, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config_param \u001b[38;5;241m:=\u001b[39m _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m    104\u001b[0m         kwargs[config_param] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    106\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool does not support sync invocation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_google_community/search.py:96\u001b[0m, in \u001b[0;36mGoogleSearchAPIWrapper.run\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run query through GoogleSearch and parse result.\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m snippets \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 96\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_google_search_results(query, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo good Google Search Result was found\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_google_community/search.py:64\u001b[0m, in \u001b[0;36mGoogleSearchAPIWrapper._google_search_results\u001b[0;34m(self, search_term, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msiterestrict:\n\u001b[1;32m     63\u001b[0m     cse \u001b[38;5;241m=\u001b[39m cse\u001b[38;5;241m.\u001b[39msiterestrict()\n\u001b[0;32m---> 64\u001b[0m res \u001b[38;5;241m=\u001b[39m cse\u001b[38;5;241m.\u001b[39mlist(q\u001b[38;5;241m=\u001b[39msearch_term, cx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoogle_cse_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mexecute()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/googleapiclient/http.py:923\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody))\n\u001b[1;32m    922\u001b[0m \u001b[38;5;66;03m# Handle retries for server-side errors.\u001b[39;00m\n\u001b[0;32m--> 923\u001b[0m resp, content \u001b[38;5;241m=\u001b[39m _retry_request(\n\u001b[1;32m    924\u001b[0m     http,\n\u001b[1;32m    925\u001b[0m     num_retries,\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sleep,\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rand,\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri),\n\u001b[1;32m    930\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod),\n\u001b[1;32m    931\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    932\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    933\u001b[0m )\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_callbacks:\n\u001b[1;32m    936\u001b[0m     callback(resp)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/googleapiclient/http.py:191\u001b[0m, in \u001b[0;36m_retry_request\u001b[0;34m(http, num_retries, req_type, sleep, rand, uri, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m     resp, content \u001b[38;5;241m=\u001b[39m http\u001b[38;5;241m.\u001b[39mrequest(uri, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Retry on SSL errors and socket timeout errors.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _ssl_SSLError \u001b[38;5;28;01mas\u001b[39;00m ssl_error:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httplib2/__init__.py:1724\u001b[0m, in \u001b[0;36mHttp.request\u001b[0;34m(self, uri, method, body, headers, redirections, connection_type)\u001b[0m\n\u001b[1;32m   1722\u001b[0m             content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1723\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1724\u001b[0m             (response, content) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1725\u001b[0m                 conn, authority, uri, request_uri, method, body, headers, redirections, cachekey,\n\u001b[1;32m   1726\u001b[0m             )\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1728\u001b[0m     is_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(e, socket\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httplib2/__init__.py:1444\u001b[0m, in \u001b[0;36mHttp._request\u001b[0;34m(self, conn, host, absolute_uri, request_uri, method, body, headers, redirections, cachekey)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auth:\n\u001b[1;32m   1442\u001b[0m     auth\u001b[38;5;241m.\u001b[39mrequest(method, request_uri, headers, body)\n\u001b[0;32m-> 1444\u001b[0m (response, content) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn_request(conn, request_uri, method, body, headers)\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auth:\n\u001b[1;32m   1447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m auth\u001b[38;5;241m.\u001b[39mresponse(response, body):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httplib2/__init__.py:1366\u001b[0m, in \u001b[0;36mHttp._conn_request\u001b[0;34m(self, conn, request_uri, method, body, headers)\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39msock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1366\u001b[0m         conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1367\u001b[0m     conn\u001b[38;5;241m.\u001b[39mrequest(method, request_uri, body, headers)\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mtimeout:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httplib2/__init__.py:1156\u001b[0m, in \u001b[0;36mHTTPSConnectionWithTimeout.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_timeout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout):\n\u001b[1;32m   1155\u001b[0m     sock\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n\u001b[0;32m-> 1156\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport))\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mwrap_socket(sock, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost)\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;66;03m# Python 3.3 compatibility: emulate the check_hostname behavior\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tool.run(\"Obama's first name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9009a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What nation hosted the Euro 2024? Include only wikipedia sources.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (call_0_edc53b20-0873-4a26-a204-8448a393fc9c)\n",
      " Call ID: call_0_edc53b20-0873-4a26-a204-8448a393fc9c\n",
      "  Args:\n",
      "    query: Euro 2024 host nation\n",
      "    include_domains: ['wikipedia.org']\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{\"query\": \"Euro 2024 host nation\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"UEFA Euro 2024 - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/UEFA_Euro_2024\", \"content\": \"It was the third time that European Championship matches were played on German territory, and the second time in reunified Germany, as West Germany hosted the 1988 tournament, and four matches of the multi-national Euro 2020 were played in Munich. Munich, the site of the first game of UEFA Euro 2024, was also a host city at the multi-national UEFA Euro 2020 tournament, hosting four matches (three involving Germany) in front of a greatly reduced number of spectators due to COVID-19 restrictions. UEFA announced the tournament schedule on 10 May 2022, which included kick-off times only for the opening match, semi-finals, and final. On 20 July 2024, after an open vote on the official Euro 2024 website, UEFA announced that Turkish defender Mert Müldür's group stage goal against Georgia had been chosen as the fans' goal of the tournament.\", \"score\": 0.85186684, \"raw_content\": null}, {\"title\": \"UEFA Euro 2024 - Simple English Wikipedia, the free encyclopedia\", \"url\": \"https://simple.wikipedia.org/wiki/UEFA_Euro_2024\", \"content\": \"| Image 4 | Many other stadiums, such as those in Bremen and Mönchengladbach were not selected.( The venues covered all the main regions of Germany, but the area with the highest number of venues at UEFA Euro 2024 is the Rhine-Ruhr metropolitan region\\\") in the state of North Rhine-Westphalia, with four of the 10 host cities (Dortmund, Düsseldorf, Gelsenkirchen and Cologne).( | Stuttgart | Image 21: UEFA Euro 2024 is located in Germany Image 22: Leipzig Berlin Image 23: Leipzig Munich Image 24: Leipzig Dortmund Image 25: Leipzig Gelsenkirchen Image 26: Leipzig Stuttgart Image 27: Leipzig Hamburg Image 28: Leipzig Düsseldorf Image 29: Leipzig Cologne Image 30: Leipzig Frankfurt Image 31: Leipzig Leipzig | Hamburg | 4.   ↑\\\"Germany and Turkey officially interested in hosting UEFA EURO 2024\\\". 5.   ↑\\\"Germany to host UEFA EURO 2024\\\".\", \"score\": 0.767429, \"raw_content\": null}, {\"title\": \"Category:Countries at UEFA Euro 2024 - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Category:Countries_at_UEFA_Euro_2024\", \"content\": \"Category:Countries at UEFA Euro 2024 - Wikipedia Main page Contents This page always uses small font size This page always uses small font size This page is always in light mode. Pages for logged out editors learn more Category:Countries at UEFA Euro 2024 Edit links Edit Edit Download QR code Edit interlanguage links Albania at UEFA Euro 2024 (3 P) Austria at UEFA Euro 2024 (4 P) Belgium at UEFA Euro 2024 (4 P) Italy at UEFA Euro 2024 (5 P) Turkey at UEFA Euro 2024 (4 P) This page was last edited on 1 January 2025, at 22:30 (UTC). Text is available under the Creative Commons Attribution-ShareAlike 4.0 License; additional terms may apply. Category:Countries at UEFA Euro 2024\", \"score\": 0.7610953, \"raw_content\": null}, {\"title\": \"UEFA Euro 2024 bids - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/UEFA_Euro_2024_bids\", \"content\": \"Previously, UEFA Euro 1988 had been held in West Germany, as well as the 1974 FIFA World Cup and the 2006 FIFA World Cup. German media has also reported that the DFB was willing to let Wembley Stadium host the final for UEFA Euro 2020, in return for support from The Football Association for Germany's 2024 bid.\", \"score\": 0.6907474, \"raw_content\": null}, {\"title\": \"Europamesterskabet i fodbold 2024 (mænd) - Wikipedia, den frie encyklopædi\", \"url\": \"https://da.wikipedia.org/wiki/Europamesterskabet_i_fodbold_2024_(mænd)\", \"content\": \"| EM i fodbold for mænd \\\"Europamesterskab (fodbold)\\\") |  Frankrig 1960  Spanien 1964  Italien 1968  Belgien 1972  Jugoslavien 1976  Italien 1980  Frankrig 1984  Vesttyskland 1988  Sverige 1992  England 1996  Belgien og Holland 2000  Portugal 2004  Østrig og Schweiz 2008  Polen og Ukraine 2012  Frankrig 2016 \\\"Europamesterskabet i fodbold 2016 (mænd)\\\")  Europa 2020 \\\"Europamesterskabet i fodbold 2020 (mænd)\\\")  Tyskland 2024  Storbritannien og Irland 2028 \\\"Europamesterskabet i fodbold 2028 (mænd)\\\")  Italien og Tyrkiet 2032 \\\"Europamesterskabet i fodbold 2032 (mænd)\\\") | | EM i fodbold Kvalifikation (herrer) |  Belgien og Holland 2000\\\")  Portugal 2004  Østrig og Schweiz 2008  Polen og Ukraine 2012  Frankrig 2016  Europa 2020  Tyskland 2024\\\") |\", \"score\": 0.65676886, \"raw_content\": null}], \"response_time\": 3.24}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Germany hosted the UEFA Euro 2024. You can find more details on the [UEFA Euro 2024 Wikipedia page](https://en.wikipedia.org/wiki/UEFA_Euro_2024).\n"
     ]
    }
   ],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=1,\n",
    "    max_tokens=1024,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize Tavily Search Tool\n",
    "tavily_search_tool = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\",\n",
    ")\n",
    "\n",
    "agent = create_react_agent(llm, [tavily_search_tool])\n",
    "\n",
    "user_input = \"What nation hosted the Euro 2024? Include only wikipedia sources.\"\n",
    "\n",
    "for step in agent.stream(\n",
    "    {\"messages\": user_input},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6f5369b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the price of 5090?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (call_0_db1ab490-3839-48ef-bd9b-fa5fe82a4e53)\n",
      " Call ID: call_0_db1ab490-3839-48ef-bd9b-fa5fe82a4e53\n",
      "  Args:\n",
      "    query: price of 5090\n",
      "    search_depth: advanced\n",
      "    time_range: week\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{\"query\": \"price of 5090\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.tomshardware.com/pc-components/gpus/cheapest-5090-ever-lucky-pc-builder-finds-rtx-5090-for-just-usd1-679-at-walmart-saving-usd1-520-card-has-worked-flawlessly-ever-since\", \"title\": \"Cheapest 5090 ever? Lucky PC builder finds RTX 5090 for 'just ...\", \"content\": \"Redditor Sufficient\\\\_Crazy7758 took to the platform to hail their purchase of the \\\"Cheapest 5090 ever?\\\", after scooping a PNY GeForce RTX 5090 from Walmart for the relatively low price of $1,679.99. The attached images reveal that the card was discounted by over $1,500 on May 25, having previously been discounted on May 13.\\n\\n\\\"Just so all of you know, I bought this on that date and it has worked flawlessly ever since,\\\" they said triumphantly. [...] In the ever-evolving world of insane GPU prices, tariffs, and frightful stock shortages, one intrepid PC builder has managed to swipe an Nvidia RTX 5090 from Walmart for \\\"just\\\" $1,679, more than $200 below MSRP. [...] even the [Founders Edition still retails for the same price](https://goto.walmart.com/c/1943169/565706/9383?subId1=tomshardware-us-9008067541337889949&sharedId=tomshardware-us&u=https%3A%2F%2Fwww.walmart.com%2Fip%2FNVIDIA-GeForce-RTX-5090-Graphic-Card-32GB-GDDR7%2F15176552726%3FclassType%3DREGULAR%26from%3D%2Fsearch), way over the $1,999 MSRP.\", \"score\": 0.84277284, \"raw_content\": null}, {\"url\": \"https://tech.yahoo.com/computing/deals/articles/cheapest-5090-ever-lucky-pc-121431363.html\", \"title\": \"Cheapest 5090 ever? Lucky PC builder finds RTX 5090 for 'just ...\", \"content\": \"Redditor Sufficient\\\\_Crazy7758 took to the platform to hail their purchase of the \\\"Cheapest 5090 ever?\\\", after scooping a PNY GeForce RTX 5090 from Walmart for the relatively low price of $1,679.99. The attached images reveal that the card was discounted by over $1,500 on May 25, having previously been discounted on May 13.\\n\\n\\\"Just so all of you know, I bought this on that date and it has worked flawlessly ever since,\\\" they said triumphantly. [...] In the ever-evolving world of insane GPU prices, tariffs, and frightful stock shortages, one intrepid PC builder has managed to swipe an Nvidia RTX 5090 from Walmart for \\\"just\\\" $1,679, more than $200 below MSRP. [...] # Cheapest 5090 ever? Lucky PC builder finds RTX 5090 for 'just' $1,679 at Walmart, saving $1,520 — card has worked 'flawlessly' ever since\\n\\n*When you buy through links on our articles, Future and its syndication partners may earn a commission.*\\n\\n![ GeForce RTX 5090. ](https://s.yimg.com/ny/api/res/1.2/dUWf9xdKU4oJblmKsWNf5g--/YXBwaWQ9aGlnaGxhbmRlcjt3PTk2MDtoPTU0MA--/https://media.zenfs.com/en/toms_hardware_319/831d0df4a9cd2d1c440dfafabaa900b5)\", \"score\": 0.84277284, \"raw_content\": null}, {\"url\": \"https://www.reddit.com/r/nvidia/comments/1l4iis7/cheapest_5090_ever/\", \"title\": \"Cheapest 5090 ever? : r/nvidia - Reddit\", \"content\": \"... RTX 509, not 5090. Upvote 845. Downvote Reply reply. Award Share. dandoorma. • 2d ago. Price should be $167.99 for any RTX 509. Upvote 204. Downvote Reply reply\", \"score\": 0.82728493, \"raw_content\": null}, {\"url\": \"https://www.msn.com/en-us/lifestyle/shopping/3-700-rtx-5090-gpus-have-found-new-homes-after-sitting-on-us-retailer-s-shelves/ar-AA1BQWrc?item=flightsprg-tipsubsc-v1a?season//4jrdwe2qca80ferr\", \"title\": \"$3,700 RTX 5090 GPUs have found new homes after sitting on US ...\", \"content\": \"Micro Center has several ROG Astral LC GeForce RTX 5090 for $3700 that have reportedly sold out.\", \"score\": 0.77792513, \"raw_content\": null}, {\"url\": \"https://www.ign.com/articles/hp-omen-max-16-geforce-rtx-5090-gaming-laptop-deal-price-drop\", \"title\": \"Grab the HP Omen Max 16 RTX 5090 Gaming Laptop for an ... - IGN\", \"content\": \"For a very limited time, HP is offering its newest and most powerful gaming laptop at a pretty hefty discount. The [Omen Max 16 GeForce RTX 5090 Gaming Laptop](https://zdcs.link/Q47vxk?object_uuid=65cdef00-ba8c-489f-b0f8-2bb56d674906&t=article) drops to just $2,559.99 with free shipping when you apply 20% off coupon code \\\"**LEVELUP20**\\\". That is easily best price for an RTX 5090 gaming laptop anywhere by over $1,000. The RTX 5090 is currently the most powerful mobile GPU available with tons of [...] **Update:** The price has dropped yet again to $2,559.99 shipped.\\n\\n## HP Omen Max 16\\\" RTX 5090 Gaming Laptop for $2,559.99\\n\\n![HP Omen Max 16\\\" Intel Core Ultra 9 275HX RTX 5090 Gaming Laptop (32GB/1TB)](https://assets-prd.ignimgs.com/2025/06/03/hpomenmax16-1748976316615.jpg?width=220&auto=webp&quality=20&dpr=0.05)\\n\\n### HP Omen Max 16\\\" Intel Core Ultra 9 275HX RTX 5090 Gaming Laptop (32GB/1TB) [...] ### Site Themes\\n\\n### Change Region\\n\\n### More\\n\\n![](https://cdn.ziffstatic.com/adchoices/adchoices.png)\\n\\n### News\\n\\n### Reviews\\n\\n### Discover\\n\\n### Videos\\n\\n### Account\\n\\n### 20Q #XX: undefined\\n\\n# Grab the HP Omen Max 16 RTX 5090 Gaming Laptop for an Incredible Price (Update: Another Big Price Drop)\\n\\n## Pick up HP's most power gaming laptop for just $2,560.\", \"score\": 0.75355774, \"raw_content\": null}], \"response_time\": 1.33}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here are the key findings regarding the price of the \"5090\" (likely referring to the NVIDIA RTX 5090 GPU):\n",
      "\n",
      "1. **Retail Price for RTX 5090 GPU**:\n",
      "   - A Redditor reported purchasing a PNY GeForce RTX 5090 from Walmart for $1,679.99, which was a significant discount of over $1,500 from its original price. The card was previously discounted on May 13 and May 25, 2025. The MSRP for the Founders Edition is mentioned as $1,999.\n",
      "   - **Source**: [Tom's Hardware](https://www.tomshardware.com/pc-components/gpus/cheapest-5090-ever-lucky-pc-builder-finds-rtx-5090-for-just-usd1-679-at-walmart-saving-usd1-520-card-has-worked-flawlessly-ever-since) and [Yahoo Tech](https://tech.yahoo.com/computing/deals/articles/cheapest-5090-ever-lucky-pc-121431363.html).\n",
      "\n",
      "2. **High-End Variants**:\n",
      "   - Micro Center listed the ROG Astral LC GeForce RTX 5090 for $3,700, which reportedly sold out.\n",
      "   - **Source**: [MSN](https://www.msn.com/en-us/lifestyle/shopping/3-700-rtx-5090-gpus-have-found-new-homes-after-sitting-on-us-retailer-s-shelves/ar-AA1BQWrc).\n",
      "\n",
      "3. **Gaming Laptop with RTX 5090**:\n",
      "   - The HP Omen Max 16 gaming laptop, equipped with an RTX 5090 GPU, was available for $2,559.99 after applying a 20% discount coupon. This price was noted as the best available for such a configuration.\n",
      "   - **Source**: [IGN](https://www.ign.com/articles/hp-omen-max-16-geforce-rtx-5090-gaming-laptop-deal-price-drop).\n",
      "\n",
      "### Summary:\n",
      "- The standalone NVIDIA RTX 5090 GPU has been sold at prices ranging from $1,679.99 (discounted) to $3,700 (high-end variant).\n",
      "- In gaming laptops, the RTX 5090 is featured in models like the HP Omen Max 16, priced at $2,559.99 after discounts. \n",
      "\n",
      "For the most accurate and up-to-date pricing, checking retailer websites like Walmart, Micro Center, or HP directly is recommended.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=1,\n",
    "    max_tokens=1024,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    )\n",
    "# Create a ReAct agent with the web searcher system prompt and Tavily search tool\n",
    "formatted_prompt = web_searcher_instructions.format(\n",
    "    current_date=get_current_date(),\n",
    "    research_topic='price of 5090',\n",
    ")\n",
    "\n",
    "# Create the agent with the formatted prompt as system message\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    [tavily_search_tool],\n",
    "    prompt=formatted_prompt\n",
    ")\n",
    "\n",
    "# Create a runnable chain from the agent\n",
    "# search_chain = agent.get_chain()\n",
    "\n",
    "# agent.invoke({\"messages\": [HumanMessage(content=\"What is the price of 5090?\")]}).pretty_print()\n",
    "\n",
    "user_input = \"What is the price of 5090?\"\n",
    "\n",
    "for step in agent.stream(\n",
    "    {\"messages\": user_input},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cea72b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from agent.tools_and_schemas import SearchQueryList, Reflection\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import AIMessage\n",
    "from langgraph.types import Send\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import START, END\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from google.genai import Client\n",
    "\n",
    "from agent.state import (\n",
    "    OverallState,\n",
    "    QueryGenerationState,\n",
    "    ReflectionState,\n",
    "    WebSearchState,\n",
    ")\n",
    "from agent.configuration import Configuration\n",
    "from agent.prompts import (\n",
    "    get_current_date,\n",
    "    query_writer_instructions,\n",
    "    web_searcher_instructions,\n",
    "    reflection_instructions,\n",
    "    answer_instructions,\n",
    ")\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "from agent.utils import (\n",
    "    get_citations,\n",
    "    get_research_topic,\n",
    "    insert_citation_markers,\n",
    "    resolve_urls,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "997dcb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_query(state: OverallState, config: RunnableConfig) -> QueryGenerationState:\n",
    "    \"\"\"LangGraph node that generates a search queries based on the User's question.\n",
    "\n",
    "    Uses Gemini 2.0 Flash to create an optimized search query for web research based on\n",
    "    the User's question.\n",
    "\n",
    "    Args:\n",
    "        state: Current graph state containing the User's question\n",
    "        config: Configuration for the runnable, including LLM provider settings\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with state update, including search_query key containing the generated query\n",
    "    \"\"\"\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "\n",
    "    # check for custom initial search query count\n",
    "    if state.get(\"initial_search_query_count\") is None:\n",
    "        state[\"initial_search_query_count\"] = configurable.number_of_initial_queries\n",
    "\n",
    "    # init Gemini 2.0 Flash\n",
    "    # llm = ChatGoogleGenerativeAI(\n",
    "    #     model=configurable.query_generator_model,\n",
    "    #     temperature=1.0,\n",
    "    #     max_retries=2,\n",
    "    #     api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    # )\n",
    "    llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=1,\n",
    "    max_tokens=1024,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    )\n",
    "    structured_llm = llm.with_structured_output(SearchQueryList)\n",
    "\n",
    "    # Format the prompt\n",
    "    current_date = get_current_date()\n",
    "    formatted_prompt = query_writer_instructions.format(\n",
    "        current_date=current_date,\n",
    "        research_topic=get_research_topic(state[\"messages\"]),\n",
    "        number_queries=state[\"initial_search_query_count\"],\n",
    "    )\n",
    "    # Generate the search queries\n",
    "    result = structured_llm.invoke(formatted_prompt)\n",
    "    return {\"query_list\": result.query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea88855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_to_web_research(state: QueryGenerationState):\n",
    "    \"\"\"LangGraph node that sends the search queries to the web research node.\n",
    "\n",
    "    This is used to spawn n number of web research nodes, one for each search query.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        Send(\"web_research\", {\"search_query\": search_query, \"id\": int(idx)})\n",
    "        for idx, search_query in enumerate(state[\"query_list\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "def web_research(state: WebSearchState, config: RunnableConfig) -> OverallState:\n",
    "    \"\"\"LangGraph node that performs web research using the native Google Search API tool.\n",
    "\n",
    "    Executes a web search using the native Google Search API tool in combination with Gemini 2.0 Flash.\n",
    "\n",
    "    Args:\n",
    "        state: Current graph state containing the search query and research loop count\n",
    "        config: Configura\"tion for the runnable, including search API settings\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with state update, including sources_gathered, research_loop_count, and web_research_results\n",
    "    \"\"\"\n",
    "    # Configure\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    formatted_prompt = web_searcher_instructions.format(\n",
    "        current_date=get_current_date(),\n",
    "        research_topic=state[\"search_query\"],\n",
    "    )\n",
    "    llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=1,\n",
    "    max_tokens=1024,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    )\n",
    "\n",
    "    # Uses the google genai client as the langchain client doesn't return grounding metadata\n",
    "    response = genai_client.models.generate_content(\n",
    "        model=configurable.query_generator_model,\n",
    "        contents=formatted_prompt,\n",
    "        config={\n",
    "            \"tools\": [{\"google_search\": {}}],\n",
    "            \"temperature\": 0,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    \n",
    "    # resolve the urls to short urls for saving tokens and time\n",
    "    resolved_urls = resolve_urls(\n",
    "        response.candidates[0].grounding_metadata.grounding_chunks, state[\"id\"]\n",
    "    )\n",
    "    # Gets the citations and adds them to the generated text\n",
    "    citations = get_citations(response, resolved_urls)\n",
    "    modified_text = insert_citation_markers(response.text, citations)\n",
    "    sources_gathered = [item for citation in citations for item in citation[\"segments\"]]\n",
    "\n",
    "    return {\n",
    "        \"sources_gathered\": sources_gathered,\n",
    "        \"search_query\": [state[\"search_query\"]],\n",
    "        \"web_research_result\": [modified_text],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef21ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(OverallState, config_schema=Configuration)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "builder.add_node(\"generate_query\", generate_query)\n",
    "builder.add_node(\"web_research\", web_research)\n",
    "builder.add_edge(START, \"generate_query\")\n",
    "builder.add_conditional_edges(\n",
    "    \"generate_query\", continue_to_web_research, [\"web_research\"]\n",
    ")\n",
    "\n",
    "builder.add_edge(\"finalize_answer\", END)\n",
    "\n",
    "graph = builder.compile(name=\"pro-search-agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1fc64",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'ModelMetaclass' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m formatted_prompt \u001b[38;5;241m=\u001b[39m web_searcher_instructions\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     16\u001b[0m     current_date\u001b[38;5;241m=\u001b[39mget_current_date(),\n\u001b[1;32m     17\u001b[0m     research_topic\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice of 5090\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m messages \u001b[38;5;241m=\u001b[39m [HumanMessage(content\u001b[38;5;241m=\u001b[39mformatted_prompt)]\n\u001b[0;32m---> 21\u001b[0m query_generation_response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m     22\u001b[0m     messages,\n\u001b[1;32m     23\u001b[0m     tools\u001b[38;5;241m=\u001b[39m[LLMGeneratedSearchQueries]\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m generated_queries \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m query_generation_response\u001b[38;5;241m.\u001b[39mtool_calls:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:372\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    368\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    369\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 372\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    373\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    374\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    375\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    376\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    377\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    378\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    379\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    380\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    381\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    382\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:957\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    955\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    956\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:776\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    775\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 776\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    777\u001b[0m                 m,\n\u001b[1;32m    778\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    779\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    780\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    781\u001b[0m             )\n\u001b[1;32m    782\u001b[0m         )\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1022\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m   1023\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1026\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:891\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     stream_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[1;32m    888\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[0;32m--> 891\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_request_payload(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    892\u001b[0m generation_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m payload:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:946\u001b[0m, in \u001b[0;36mBaseChatOpenAI._get_request_payload\u001b[0;34m(self, input_, stop, **kwargs)\u001b[0m\n\u001b[1;32m    943\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n\u001b[1;32m    945\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_responses_api(payload):\n\u001b[1;32m    947\u001b[0m     payload \u001b[38;5;241m=\u001b[39m _construct_responses_api_payload(messages, payload)\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:932\u001b[0m, in \u001b[0;36mBaseChatOpenAI._use_responses_api\u001b[0;34m(self, payload)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_responses_api\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _use_responses_api(payload)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:2866\u001b[0m, in \u001b[0;36m_use_responses_api\u001b[0;34m(payload)\u001b[0m\n\u001b[1;32m   2865\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_use_responses_api\u001b[39m(payload: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m-> 2866\u001b[0m     uses_builtin_tools \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m payload \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   2867\u001b[0m         _is_builtin_tool(tool) \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2868\u001b[0m     )\n\u001b[1;32m   2869\u001b[0m     responses_only_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_response_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(uses_builtin_tools \u001b[38;5;129;01mor\u001b[39;00m responses_only_args\u001b[38;5;241m.\u001b[39mintersection(payload))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:2867\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2865\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_use_responses_api\u001b[39m(payload: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   2866\u001b[0m     uses_builtin_tools \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m payload \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m-> 2867\u001b[0m         _is_builtin_tool(tool) \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2868\u001b[0m     )\n\u001b[1;32m   2869\u001b[0m     responses_only_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_response_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(uses_builtin_tools \u001b[38;5;129;01mor\u001b[39;00m responses_only_args\u001b[38;5;241m.\u001b[39mintersection(payload))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:2862\u001b[0m, in \u001b[0;36m_is_builtin_tool\u001b[0;34m(tool)\u001b[0m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_builtin_tool\u001b[39m(tool: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m-> 2862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tool \u001b[38;5;129;01mand\u001b[39;00m tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'ModelMetaclass' is not iterable"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "class LLMGeneratedSearchQueries(BaseModel):\n",
    "    \"\"\"Schema for the LLM to generate a list of search queries.\"\"\"\n",
    "    search_queries: list[str] = Field(description=\"A list of search queries to find relevant information based on the research topic.\")\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=1024,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "    )\n",
    "\n",
    "messages = [HumanMessage(content=formatted_prompt)]\n",
    "\n",
    "query_generation_response = llm.invoke(\n",
    "    messages,\n",
    "    tools=[LLMGeneratedSearchQueries]\n",
    ")\n",
    "\n",
    "generated_queries = []\n",
    "if query_generation_response.tool_calls:\n",
    "    for tool_call in query_generation_response.tool_calls:\n",
    "        if tool_call['name'] == LLMGeneratedSearchQueries.__name__:\n",
    "            generated_queries.extend(tool_call['args'].get('search_queries', []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34382436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
