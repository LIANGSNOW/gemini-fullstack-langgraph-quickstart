{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f633dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from google import genai\n",
    "\n",
    "# pass your API key here\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\") or getpass(\n",
    "    \"Enter Google API Key: \"\n",
    ")\n",
    "# initialize our client\n",
    "client = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a42bbec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Gemini refers to a few different things, but most commonly it refers to **Google's large language model.** Here's a breakdown:\n",
       "\n",
       "**1. Gemini (The Google AI Model):**\n",
       "\n",
       "*   **What it is:** Gemini is a multimodal AI model developed by Google AI. It's designed to be Google's most capable and general AI model, surpassing previous models like LaMDA and PaLM.\n",
       "*   **Capabilities:** It can understand and generate text, images, audio, video, and code. This \"multimodal\" nature means it can process and understand information from various sources simultaneously. It's designed to be highly adaptable and perform well on a wide range of tasks.\n",
       "*   **Versions:** There are different versions of Gemini optimized for various use cases:\n",
       "    *   **Gemini Ultra:** The most powerful version, designed for highly complex tasks.\n",
       "    *   **Gemini Pro:** A more balanced version, suitable for a wide range of applications, including powering Google's Bard (now Gemini) chatbot.\n",
       "    *   **Gemini Nano:** A lightweight version designed to run on devices like smartphones and other edge devices.\n",
       "*   **Use Cases:** Gemini is being integrated into various Google products and services, including:\n",
       "    *   **Search:** Improving search results and providing more comprehensive answers.\n",
       "    *   **Gmail & Workspace:** Helping with email composition, summarizing documents, and generating ideas.\n",
       "    *   **Bard/Gemini Chatbot:** Providing more natural and helpful conversational AI.\n",
       "    *   **Coding:** Assisting developers with code generation, debugging, and understanding code.\n",
       "    *   **Creative Content:** Helping with writing, image generation, and other creative tasks.\n",
       "\n",
       "**2. Gemini (The Zodiac Sign):**\n",
       "\n",
       "*   In astrology, Gemini is the third sign of the zodiac, associated with people born between approximately May 21 and June 20. It's an air sign often associated with communication, curiosity, and adaptability.\n",
       "\n",
       "**3. Project Gemini (NASA):**\n",
       "\n",
       "*   In the 1960s, Project Gemini was a NASA human spaceflight program that served as a bridge between Project Mercury (the first US human spaceflight program) and Project Apollo (the program that landed humans on the Moon). Gemini developed techniques for rendezvous, docking, and spacewalking that were crucial for the success of Apollo.\n",
       "\n",
       "**In summary, when someone mentions \"Gemini\" today, they are most likely referring to the Google AI model.**  To avoid confusion, it's helpful to consider the context in which the word is used.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "model_id = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=model_id, contents='What is Gemini?'\n",
    ")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28cc352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai.types import Tool, GoogleSearch\n",
    "\n",
    "search_tool = Tool(google_search=GoogleSearch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef846338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentConfig(http_options=None, system_instruction='You are a helpful assistant that provides up to date information to help the user in their research.', temperature=0.0, top_p=None, top_k=None, candidate_count=1, max_output_tokens=None, stop_sequences=None, response_logprobs=None, logprobs=None, presence_penalty=None, frequency_penalty=None, seed=None, response_mime_type=None, response_schema=None, routing_config=None, model_selection_config=None, safety_settings=None, tools=[Tool(retrieval=None, google_search=GoogleSearch(), google_search_retrieval=None, enterprise_web_search=None, google_maps=None, code_execution=None, function_declarations=None)], tool_config=None, labels=None, cached_content=None, response_modalities=['TEXT'], media_resolution=None, speech_config=None, audio_timestamp=None, automatic_function_calling=None, thinking_config=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.genai.types import GenerateContentConfig\n",
    "model_id = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "config = GenerateContentConfig(\n",
    "    system_instruction=(\n",
    "        \"You are a helpful assistant that provides up to date information \"\n",
    "        \"to help the user in their research.\"\n",
    "    ),\n",
    "    tools=[search_tool],\n",
    "    response_modalities=[\"TEXT\"],\n",
    "    temperature=0.0,  # likely default\n",
    "    candidate_count=1,  # likely default\n",
    "\n",
    ")\n",
    "config\n",
    "response = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    contents=\"Tell me the latest news in AI\",\n",
    "    config=config\n",
    ")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da03f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    contents=\"Tell me the latest news in AI\",\n",
    "    config=config\n",
    ")\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a33b00b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.0-flash-exp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     contents\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me the latest news in AI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     config\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle_search\u001b[39m\u001b[38;5;124m\"\u001b[39m: {}}],\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      7\u001b[0m         },\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m      9\u001b[0m Markdown(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/google/genai/models.py:5202\u001b[0m, in \u001b[0;36mModels.generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5200\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   5201\u001b[0m   i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 5202\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_content(\n\u001b[1;32m   5203\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel, contents\u001b[38;5;241m=\u001b[39mcontents, config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[1;32m   5204\u001b[0m   )\n\u001b[1;32m   5205\u001b[0m   logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is done.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   5206\u001b[0m   remaining_remote_calls_afc \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/google/genai/models.py:4178\u001b[0m, in \u001b[0;36mModels._generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4175\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mconvert_to_dict(request_dict)\n\u001b[1;32m   4176\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[0;32m-> 4178\u001b[0m response_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m   4179\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m, path, request_dict, http_options\n\u001b[1;32m   4180\u001b[0m )\n\u001b[1;32m   4182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mvertexai:\n\u001b[1;32m   4183\u001b[0m   response_dict \u001b[38;5;241m=\u001b[39m _GenerateContentResponse_from_vertex(\n\u001b[1;32m   4184\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client, response_dict\n\u001b[1;32m   4185\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/google/genai/_api_client.py:755\u001b[0m, in \u001b[0;36mBaseApiClient.request\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    747\u001b[0m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    750\u001b[0m     http_options: Optional[HttpOptionsOrDict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    751\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[BaseResponse, Any]:\n\u001b[1;32m    752\u001b[0m   http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[1;32m    753\u001b[0m       http_method, path, request_dict, http_options\n\u001b[1;32m    754\u001b[0m   )\n\u001b[0;32m--> 755\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(http_request, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    756\u001b[0m   json_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson\n\u001b[1;32m    757\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/google/genai/_api_client.py:677\u001b[0m, in \u001b[0;36mBaseApiClient._request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    673\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[1;32m    674\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[1;32m    675\u001b[0m   )\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 677\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpx_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    678\u001b[0m       method\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    679\u001b[0m       url\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    680\u001b[0m       headers\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    681\u001b[0m       content\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    682\u001b[0m       timeout\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    683\u001b[0m   )\n\u001b[1;32m    684\u001b[0m   errors\u001b[38;5;241m.\u001b[39mAPIError\u001b[38;5;241m.\u001b[39mraise_for_response(response)\n\u001b[1;32m    685\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[1;32m    686\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[1;32m    687\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:825\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    810\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    812\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    813\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    814\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    823\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    824\u001b[0m )\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(request, auth\u001b[38;5;241m=\u001b[39mauth, follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m    915\u001b[0m     request,\n\u001b[1;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    919\u001b[0m )\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m    943\u001b[0m         request,\n\u001b[1;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection.py:99\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection.py:76\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_lock:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m         stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect(request)\n\u001b[1;32m     78\u001b[0m         ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m         http2_negotiated \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     80\u001b[0m             ssl_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     81\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m ssl_object\u001b[38;5;241m.\u001b[39mselected_alpn_protocol() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection.py:122\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    114\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhost\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_origin\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_origin\u001b[38;5;241m.\u001b[39mport,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocket_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket_options,\n\u001b[1;32m    120\u001b[0m     }\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 122\u001b[0m         stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_backend\u001b[38;5;241m.\u001b[39mconnect_tcp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    123\u001b[0m         trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/httpcore/_backends/sync.py:206\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    200\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    201\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    203\u001b[0m }\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m--> 206\u001b[0m     sock \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    207\u001b[0m         address,\n\u001b[1;32m    208\u001b[0m         timeout,\n\u001b[1;32m    209\u001b[0m         source_address\u001b[38;5;241m=\u001b[39msource_address,\n\u001b[1;32m    210\u001b[0m     )\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m socket_options:\n\u001b[1;32m    212\u001b[0m         sock\u001b[38;5;241m.\u001b[39msetsockopt(\u001b[38;5;241m*\u001b[39moption)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:836\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m    835\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m--> 836\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m    838\u001b[0m exceptions\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    contents=\"Tell me the latest news in AI\",\n",
    "    config={\n",
    "        \"tools\": [{\"google_search\": {}}],\n",
    "        \"temperature\": 0,\n",
    "        },\n",
    "    )\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9009a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='The nation that hosted the UEFA Euro 2024 was Germany ([Wikipedia](https://en.wikipedia.org/wiki/UEFA_Euro_2024)).' citations=[Citation(source_id='https://en.wikipedia.org/wiki/UEFA_Euro_2024', quote=\"UEFA Euro 2024 was the 17th edition of the UEFA European Championship, the quadrennial international men's football championship of Europe organized by UEFA. It took place in Germany from 14 June to 14 July 2024.\")]\n"
     ]
    }
   ],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, List, Annotated, Literal, Dict, Union, Optional \n",
    "\n",
    "class Citation(BaseModel):\n",
    "    source_id: str = Field(\n",
    "        ...,\n",
    "        description=\"The url of a SPECIFIC source which justifies the answer.\",\n",
    "    )\n",
    "    quote: str = Field(\n",
    "        ...,\n",
    "        description=\"The VERBATIM quote from the specified source that justifies the answer.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class QuotedAnswer(BaseModel):\n",
    "    \"\"\"Answer the user question based only on the given sources, and cite the sources used.\"\"\"\n",
    "    answer: str = Field(\n",
    "        ...,\n",
    "        description=\"The answer to the user question, which is based only on the given sources. Include any relevant sources in the answer as markdown hyperlinks. For example: 'This is a sample text ([url website](url))'\"\n",
    "    )\n",
    "    citations: List[Citation] = Field(\n",
    "        ..., description=\"Citations from the given sources that justify the answer.\"\n",
    "    )\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=1,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize Tavily Search Tool\n",
    "tavily_search_tool = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\",\n",
    ")\n",
    "structured_llm = llm.bind_tools([tavily_search_tool]).with_structured_output(QuotedAnswer)\n",
    "# agent = create_react_agent(llm, [tavily_search_tool])\n",
    "\n",
    "user_input = \"What nation hosted the Euro 2024? Include only wikipedia sources.\"\n",
    "\n",
    "# for step in agent.stream(\n",
    "#     {\"messages\": user_input},\n",
    "#     stream_mode=\"values\",\n",
    "# ):\n",
    "#     step[\"messages\"][-1].pretty_print()\n",
    "result = structured_llm.invoke(user_input)\n",
    "print(result)\n",
    "# https://github.com/tavily-ai/use-cases/blob/main/company-research/company_research.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f5369b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Conduct targeted Google Searches to gather the most recent, credible information on \"price of 5090\" and synthesize it into a verifiable text artifact.\n",
      "\n",
      "Instructions:\n",
      "- Query should ensure that the most current information is gathered. The current date is June 11, 2025.\n",
      "- Conduct multiple, diverse searches to gather comprehensive information.\n",
      "- Consolidate key findings while meticulously tracking the source(s) for each specific piece of information.\n",
      "- The output should be a well-written summary or report based on your search findings. \n",
      "- Only include the information found in the search results, don't make up any information.\n",
      "\n",
      "Research Topic:\n",
      "price of 5090\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (call_0_2bb390f4-2652-4cf9-b4ab-d5d4fd777f6e)\n",
      " Call ID: call_0_2bb390f4-2652-4cf9-b4ab-d5d4fd777f6e\n",
      "  Args:\n",
      "    query: price of NVIDIA RTX 5090 June 2025\n",
      "    search_depth: advanced\n",
      "    time_range: week\n",
      "  tavily_search (call_1_c2e1a03b-d2f0-4ef0-807f-64791272807e)\n",
      " Call ID: call_1_c2e1a03b-d2f0-4ef0-807f-64791272807e\n",
      "  Args:\n",
      "    query: latest price of RTX 5090 graphics card June 2025\n",
      "    search_depth: advanced\n",
      "    time_range: week\n",
      "  tavily_search (call_2_9dd83a9c-5d5d-474f-a590-e275cb5106a4)\n",
      " Call ID: call_2_9dd83a9c-5d5d-474f-a590-e275cb5106a4\n",
      "  Args:\n",
      "    query: RTX 5090 release date and price June 2025\n",
      "    search_depth: advanced\n",
      "    time_range: week\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{\"query\": \"RTX 5090 release date and price June 2025\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://pangoly.com/en/compare/vga/gigabyte-radeon-rx-9060-xt-gaming-oc-8gb-vs-msi-geforce-rtx-5090-32g-vanguard-soc-launch-edition\", \"title\": \"MSI GeForce RTX 5090 32G VANGUARD SOC LAUNCH EDITION\", \"content\": \"Release Date. June 2025. Release Date. January 2025. * First availability for purchase, it may not correspond to the actual market launch date. User\", \"score\": 0.80014634, \"raw_content\": null}, {\"url\": \"https://www.extremetech.com/deals/best-graphics-card-gpu-deals\", \"title\": \"Best Graphics Card (GPU) Deals for June 2025 | Extremetech\", \"content\": \"Gigabyte GeForce RTX 5070 Graphics Card for $699.99 (list price $819.99). MSI GeForce RTX 5090 Trio Graphics Card for $3,305.60 (list price $3,968.99). EVGA\", \"score\": 0.7882741, \"raw_content\": null}, {\"url\": \"https://finalboss.io/asus-rog-astral-dhahab-edition-geforce-rtx-5090-3\", \"title\": \"Asus ROG Astral Dhahab Edition GeForce RTX 5090 - FinalBoss\", \"content\": \"![FinalBoss.io](/_next/image?url=%2Ffinalboss.png&w=256&q=75)\\n![Asus ROG Astral Dhahab Edition GeForce RTX 5090: Gold, Glamour, and a $10,600 Price Tag](/_next/image?url=https%3A%2F%2Fbackend.finalboss.io%2Fwp-content%2Fuploads%2F2025%2F05%2Ftmpymc5znng_tenant1_ext_dd7dfd0bdc1e.png&w=3840&q=75)\\n\\n# Asus ROG Astral Dhahab Edition GeForce RTX 5090: Gold, Glamour, and a $10,600 Price Tag\\n\\n## Asus ROG Astral Dhahab Edition GeForce RTX 5090: The $10,600 Golden GPU [...] Graphics cards have always had their share of special editions-think flashy RGB, custom cooling, and collaborations that toe the line between art and excess. But Asus has turned the dial to eleven with the **ROG Astral Dhahab Edition GeForce RTX 5090**, draping Nvidia’s most powerful consumer GPU in 6.5 grams of pure 24-carat gold. The price? A jaw-dropping 38,800 AED (about $10,600), making it a bona fide monument to GPU luxury in the Middle East. [...] | Feature | Specification |\\n| --- | --- |\\n| Feature | Specification |\\n| Model | Asus ROG Astral Dhahab Edition GeForce RTX 5090 |\\n| GPU | Nvidia GeForce RTX 5090 |\\n| VRAM | TBA |\\n| Boost Clock | TBA |\\n| Gold Plating | 6.5g 999-fineness (24-carat) |\\n| Region | Middle East Exclusive |\\n| MSRP | 38,800 AED (approx. $10,600) |\", \"score\": 0.767429, \"raw_content\": null}, {\"url\": \"https://thepcenthusiast.com/gigabyte-aorus-geforce-rtx-5090-stealth-ice-graphics-card-has-hidden-power-connector/\", \"title\": \"Gigabyte AORUS GeForce RTX 5090 Stealth Ice Graphics Card Has ...\", \"content\": \"| **Video Engines** | 3 x NVENC (9th Gen)  2 x NVDEC (6th Gen) | 2 x NVENC (9th Gen)  2 x NVDEC (6th Gen) | 2 x NVENC (9th Gen)  1 x NVDEC (6th Gen) | 1 x NVENC (9th Gen)  1 x NVDEC (6th Gen) | 1 x NVENC (9th Gen)  1 x NVDEC (6th Gen) |\\n| **TGP (Total Graphics Power)** | 575 W | 360 W | 300 W | 250 W | 180 W |\\n| **Price (Launch)** | $1,999 | $999 | $749 | $549 | $429 / $379 |\\n| **Release Date** | Jan. 2025 | Jan. 2025 | Feb. 2025 | Feb. 2025 | April 2025 | [...] Gigabyte has not included the manufacturer’s suggested retail price or the exact release date of their new RTX 5090 Stealth Ice graphics card. But considering that the RTX 5090 Master ICE retails for around $3,000, we expect that the new model would be priced similarly. [...] ![AORUS GeForce RTX 5090 STEALTH ICE -02](data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20980%20551'%3E%3C/svg%3E)\\n![AORUS GeForce RTX 5090 STEALTH ICE -02](https://cdn.thepcenthusiast.com/wp-content/uploads/2025/06/AORUS-GeForce-RTX-5090-STEALTH-ICE-02-980x551.jpg)\\n\\n## Pricing and Availability\", \"score\": 0.74229145, \"raw_content\": null}, {\"url\": \"https://www.tomshardware.com/pc-components/gpus/cheapest-5090-ever-lucky-pc-builder-finds-rtx-5090-for-just-usd1-679-at-walmart-saving-usd1-520-card-has-worked-flawlessly-ever-since\", \"title\": \"Cheapest 5090 ever? Lucky PC builder finds RTX 5090 for 'just ...\", \"content\": \"Published Time: 2025-06-06T12:14:31+00:00\\n\\nCheapest 5090 ever? Lucky PC builder finds RTX 5090 for 'just' $1,679 at Walmart, saving $1,520 — card has worked 'flawlessly' ever since | Tom's Hardware\\n\\n===============\\n[Skip to main content](https://www.tomshardware.com/pc-components/gpus/cheapest-5090-ever-lucky-pc-builder-finds-rtx-5090-for-just-usd1-679-at-walmart-saving-usd1-520-card-has-worked-flawlessly-ever-since#main)\\n\\nOpen menu Close menu \\n\\n[Tom's Hardware](https://www.tomshardware.com/) [...] $4,000](https://www.tomshardware.com/pc-components/gpus/data-suggests-rtx-5090-prices-are-slowly-stabilizing-the-average-selling-price-on-ebay-has-dropped-to-usd4-000)[![Image 19: Gigabyte RTX 5090 plus Z890 motherboard bundle at Newegg](https://cdn.mos.cms.futurecdn.net/vPgi9VDhLgVNP2YzbqU8wP.jpg) Newegg has RTX 5090 bundles starting at 'only' $3,139](https://www.tomshardware.com/pc-components/gpus/newegg-has-rtx-5090-bundles-starting-at-usd3-139-counts-as-a-good-gpu-deal-these-days)[![Image [...] *   [Where to Buy RTX 5060](https://www.tomshardware.com/pc-components/gpus/where-to-buy-nvidias-rtx-5060-8gb-gpu)\\n*   [AMD Radeon RX 9060 XT](https://www.tomshardware.com/pc-components/gpus/amd-radeon-rx-9060-xt-launches-on-june-5-starting-at-usd299)\\n*   [Switch 2](https://www.tomshardware.com/video-games/nintendo/nintendo-switch-2-hands-on-bigger-faster-and-with-mouse-controls)\", \"score\": 0.70531887, \"raw_content\": null}], \"response_time\": 2.88}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here is a synthesized report on the latest information about the price of the NVIDIA RTX 5090 as of June 2025, based on credible sources:\n",
      "\n",
      "### **Summary of RTX 5090 Pricing (June 2025)**\n",
      "1. **Standard Models**:\n",
      "   - The **MSI GeForce RTX 5090 Trio** is listed at **$3,305.60** (down from a list price of $3,968.99) on Extremetech ([Source](https://www.extremetech.com/deals/best-graphics-card-gpu-deals)).\n",
      "   - A lucky buyer found a **PNY GeForce RTX 5090** at Walmart for **$1,679.99**, a significant discount from its original price ([Source](https://www.tomshardware.com/pc-components/gpus/cheapest-5090-ever-lucky-pc-builder-finds-rtx-5090-for-just-usd1-679-at-walmart-saving-usd1-520-card-has-worked-flawlessly-ever-since)).\n",
      "   - The **expected MSRP** for a standard RTX 5090 is anticipated to be between **$1,500 and $2,000**, though prices vary by retailer and model ([Source](https://finalboss.io/asus-rog-astral-dhahab-edition-geforce-rtx-5090-3)).\n",
      "\n",
      "2. **Special Editions**:\n",
      "   - The **Asus ROG Astral Dhahab Edition** (a luxury variant with 24-carat gold plating) is priced at **$10,600** and is exclusive to the Middle East ([Source](https://finalboss.io/asus-rog-astral-dhahab-edition-geforce-rtx-5090-3)).\n",
      "\n",
      "3. **Retailer-Specific Deals**:\n",
      "   - **Best Buy** offered the **ASUS TUF Gaming RTX 5090** for **$2,999.99** ([Source](https://x.com/RTX50Drops/status/1932092018026516544)).\n",
      "   - **Newegg** has RTX 5090 bundles starting at **$3,139** ([Source](https://www.tomshardware.com/pc-components/gpus/newegg-has-rtx-5090-bundles-starting-at-usd3-139-counts-as-a-good-gpu-deal-these-days)).\n",
      "\n",
      "4. **Market Trends**:\n",
      "   - Prices on eBay have stabilized around **$4,000** for high-demand models ([Source](https://www.tomshardware.com/pc-components/gpus/data-suggests-rtx-5090-prices-are-slowly-stabilizing-the-average-selling-price-on-ebay-has-dropped-to-usd4-000)).\n",
      "   - Some Reddit users report that most RTX 5090 models are priced **above $3,000**, with older models like the RTX 4090 still selling for around **$2,200** ([Source](https://www.reddit.com/r/nvidia/comments/1l5w8ez/which_rtx_5090_models_are_actually_worth_it_in/)).\n",
      "\n",
      "### **Key Takeaways**\n",
      "- The RTX 5090 is a high-end GPU with prices ranging from **$1,500 to over $10,000** for special editions.\n",
      "- Retailers like Walmart and Best Buy occasionally offer significant discounts, but most models remain expensive.\n",
      "- The market shows signs of stabilization, but demand keeps prices elevated.\n",
      "\n",
      "This report is based on verified sources as of June 2025. For the most current deals, checking retailer websites directly is recommended.\n"
     ]
    }
   ],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from dotenv import load_dotenv\n",
    "from backend.src.agent.prompts import (\n",
    "    get_current_date,\n",
    "    query_writer_instructions,\n",
    "    web_searcher_instructions,\n",
    "    reflection_instructions,\n",
    "    answer_instructions,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=1,\n",
    "    max_tokens=1024,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    )\n",
    "# Create a ReAct agent with the web searcher system prompt and Tavily search tool\n",
    "formatted_prompt = web_searcher_instructions.format(\n",
    "    current_date=get_current_date(),\n",
    "    research_topic='price of 5090',\n",
    ")\n",
    "tavily_search_tool = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\",\n",
    ")\n",
    "# Create the agent with the formatted prompt as system message\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    [tavily_search_tool],\n",
    "    \n",
    ")\n",
    "prompt=formatted_prompt\n",
    "# Create a runnable chain from the agent\n",
    "# search_chain = agent.get_chain()\n",
    "\n",
    "# agent.invoke({\"messages\": [HumanMessage(content=\"What is the price of 5090?\")]}).pretty_print()\n",
    "\n",
    "# user_input = \"What is the price of 5090?\"\n",
    "# response = agent.invoke({\"messages\": prompt})\n",
    "for step in agent.stream(\n",
    "    {\"messages\": prompt},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1b97f1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"query\": \"latest updates on RTX 5090 pricing June 2025\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://x.com/USRestocks/status/1932361271782863013\", \"title\": \"US Restocks RTX5090/60/60Ti § Switch2\", \"content\": \"RTX 5090 in Stock! | Cart Link: https://amzn.to/4bPc14X June 10, 2025 at 01:56AM PT #ad MSI Gaming RTX 5090 32G Gaming Trio OC Graphics Card (32GB GDDR7,\", \"score\": 0.79254496, \"raw_content\": null}, {\"url\": \"https://www.youtube.com/watch?v=l641VkblkfY\", \"title\": \"Best GPUs to Buy for Gaming RIGHT NOW! [June/July 2025]\", \"content\": \"Newegg (US) [PAID LINKS]:\\\\nB570: https://newegg.io/nc0f72caf3\\\\nB580: https://newegg.io/ncb8aa2818\\\\nRTX 5060: https://newegg.io/nc2d2d6cb4\\\\nRTX 5060 Ti 16GB: https://newegg.io/nce151063f\\\\nRX 9070: https://newegg.io/nc6c904af2\\\\nRTX 5070 Ti: https://newegg.io/ncc8d843aa\\\\nRX 9070 XT: https://newegg.io/ncd7924039\\\\nRTX 5080: https://newegg.io/nc45487d71\\\\nRTX 5090: https://newegg.io/nc60c8042e [...] Amazon (UK) [PAID LINKS]:\\\\nB570: https://amzlink.to/az0zJQpeqq2Js\\\\nB580: https://amzlink.to/az0LAlIxFBfKo\\\\nRTX 5060: https://amzlink.to/az0hIVbEZrI6B\\\\nRTX 5060 Ti 16GB: https://amzlink.to/az0HkaR5MIpvR\\\\nRX 9070: https://amzlink.to/az0syS72HTuPw\\\\nRTX 5070 Ti: https://amzlink.to/az0pZ6PXayEAU\\\\nRX 9070 XT: https://amzlink.to/az0LwqrQy9bK1\\\\nRTX 5080: https://amzlink.to/az0n4hQDRq9rx\\\\nRTX 5090: https://amzlink.to/az0HXuNlzKZWX [...] Amazon (US) [PAID LINKS]:\\\\nB570: https://amzlink.to/az0HOxxNV4OF8\\\\nB580: https://amzlink.to/az0u4cH6r0fB2\\\\nRTX 5060: https://amzlink.to/az0zbM77NOV2Z\\\\nRTX 5060 Ti 16GB: https://amzlink.to/az0YZkOnMFXY3\\\\nRX 9070: https://amzlink.to/az03UwkWLMGmH\\\\nRTX 5070 Ti: https://amzlink.to/az0AItZVPbPNO\\\\nRX 9070 XT: https://amzlink.to/az0oOI9UxwEay\\\\nRTX 5080: https://amzlink.to/az0SSNK8AQuzo\\\\nRTX 5090: https://amzlink.to/az0aj8ecrctEC\", \"score\": 0.73023266, \"raw_content\": null}, {\"url\": \"https://en.gamegpu.com/iron/msi-and-gigabyte-are-delaying-rtx-5090-deliveries-to-susa-until-power-rates-are-up-by-july-9\", \"title\": \"MSI and Gigabyte RTX 5090 Shipments to US Ahead of Tariffs ...\", \"content\": \"# MSI and Gigabyte RTX 5090 Shipments to US Ahead of Tariffs Coming into Effect on July 9\\\\n\\\\nAuthor [Maximum Games](https://en.gamegpu.com/view-user-profile?user=41287), 10 2025 June. Published in [Hardware](/backgammon/news/iron/) [...] ![rtx 5090 gigabyte](https://gamegpu.com/images/1_2025/NEWS/q2/f123/9600/rtx_5090_gigabyte.jpg)\\\\n\\\\n![rtx 5090 gigabyte](https://gamegpu.com/images/1_2025/NEWS/q2/f123/9600/rtx_5090_gigabyte.jpg) [...] Experts suggest that **NVIDIA\\'s AIB Partners Will Hold Back RTX 5090 Stock**, waiting for the official return of tariffs. This will give the opportunity to raise prices after the deadline if agreements between the countries are not reached. A similar scheme was used in 2016-2017, when companies massively imported video cards before the start of trade restrictions. **RTX 5090 Availability May Worsen in July**, especially if tariffs are reintroduced and companies decide to pass the costs on to\", \"score\": 0.65173227, \"raw_content\": null}, {\"url\": \"https://www.notebookcheck.net/2025-Lenovo-Legion-Pro-7i-with-RTX-5090-and-Core-Ultra-9-275HX-gets-another-price-cut.1033540.0.html\", \"title\": \"2025 Lenovo Legion Pro 7i with RTX 5090 and Core Ultra 9 275HX ...\", \"content\": \"Deal | 2025 Lenovo Legion Pro 7i with RTX 5090 and Core Ultra 9 275HX gets another price cut ... It goes without saying that the top-end RTX 5090 configuration of\", \"score\": 0.64719695, \"raw_content\": null}, {\"url\": \"https://www.tomshardware.com/pc-components/gpus/cheapest-5090-ever-lucky-pc-builder-finds-rtx-5090-for-just-usd1-679-at-walmart-saving-usd1-520-card-has-worked-flawlessly-ever-since\", \"title\": \"Cheapest 5090 ever? Lucky PC builder finds RTX 5090 for \\'just ...\", \"content\": \"Redditor Sufficient\\\\\\\\_Crazy7758 took to the platform to hail their purchase of the \\\\\"Cheapest 5090 ever?\\\\\", after scooping a PNY GeForce RTX 5090 from Walmart for the relatively low price of $1,679.99. The attached images reveal that the card was discounted by over $1,500 on May 25, having previously been discounted on May 13.\\\\n\\\\n\\\\\"Just so all of you know, I bought this on that date and it has worked flawlessly ever since,\\\\\" they said triumphantly. [...] In the ever-evolving world of insane GPU prices, tariffs, and frightful stock shortages, one intrepid PC builder has managed to swipe an Nvidia RTX 5090 from Walmart for \\\\\"just\\\\\" $1,679, more than $200 below MSRP. [...] even the [Founders Edition still retails for the same price](https://goto.walmart.com/c/1943169/565706/9383?subId1=tomshardware-us-9572975325724004555&sharedId=tomshardware-us&u=https%3A%2F%2Fwww.walmart.com%2Fip%2FNVIDIA-GeForce-RTX-5090-Graphic-Card-32GB-GDDR7%2F15176552726%3FclassType%3DREGULAR%26from%3D%2Fsearch), way over the $1,999 MSRP.\", \"score\": 0.6016175, \"raw_content\": null}], \"response_time\": 2.98}'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['messages'][4].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a53d7a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\"query\": \"latest updates on RTX 5090 pricing June 2025\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://x.com/USRestocks/status/1932361271782863013\", \"title\": \"US Restocks RTX5090/60/60Ti § Switch2\", \"content\": \"RTX 5090 in Stock! | Cart Link: https://amzn.to/4bPc14X June 10, 2025 at 01:56AM PT #ad MSI Gaming RTX 5090 32G Gaming Trio OC Graphics Card (32GB GDDR7,\", \"score\": 0.79254496, \"raw_content\": null}, {\"url\": \"https://www.youtube.com/watch?v=l641VkblkfY\", \"title\": \"Best GPUs to Buy for Gaming RIGHT NOW! [June/July 2025]\", \"content\": \"Newegg (US) [PAID LINKS]:\\nB570: https://newegg.io/nc0f72caf3\\nB580: https://newegg.io/ncb8aa2818\\nRTX 5060: https://newegg.io/nc2d2d6cb4\\nRTX 5060 Ti 16GB: https://newegg.io/nce151063f\\nRX 9070: https://newegg.io/nc6c904af2\\nRTX 5070 Ti: https://newegg.io/ncc8d843aa\\nRX 9070 XT: https://newegg.io/ncd7924039\\nRTX 5080: https://newegg.io/nc45487d71\\nRTX 5090: https://newegg.io/nc60c8042e [...] Amazon (UK) [PAID LINKS]:\\nB570: https://amzlink.to/az0zJQpeqq2Js\\nB580: https://amzlink.to/az0LAlIxFBfKo\\nRTX 5060: https://amzlink.to/az0hIVbEZrI6B\\nRTX 5060 Ti 16GB: https://amzlink.to/az0HkaR5MIpvR\\nRX 9070: https://amzlink.to/az0syS72HTuPw\\nRTX 5070 Ti: https://amzlink.to/az0pZ6PXayEAU\\nRX 9070 XT: https://amzlink.to/az0LwqrQy9bK1\\nRTX 5080: https://amzlink.to/az0n4hQDRq9rx\\nRTX 5090: https://amzlink.to/az0HXuNlzKZWX [...] Amazon (US) [PAID LINKS]:\\nB570: https://amzlink.to/az0HOxxNV4OF8\\nB580: https://amzlink.to/az0u4cH6r0fB2\\nRTX 5060: https://amzlink.to/az0zbM77NOV2Z\\nRTX 5060 Ti 16GB: https://amzlink.to/az0YZkOnMFXY3\\nRX 9070: https://amzlink.to/az03UwkWLMGmH\\nRTX 5070 Ti: https://amzlink.to/az0AItZVPbPNO\\nRX 9070 XT: https://amzlink.to/az0oOI9UxwEay\\nRTX 5080: https://amzlink.to/az0SSNK8AQuzo\\nRTX 5090: https://amzlink.to/az0aj8ecrctEC\", \"score\": 0.73023266, \"raw_content\": null}, {\"url\": \"https://en.gamegpu.com/iron/msi-and-gigabyte-are-delaying-rtx-5090-deliveries-to-susa-until-power-rates-are-up-by-july-9\", \"title\": \"MSI and Gigabyte RTX 5090 Shipments to US Ahead of Tariffs ...\", \"content\": \"# MSI and Gigabyte RTX 5090 Shipments to US Ahead of Tariffs Coming into Effect on July 9\\n\\nAuthor [Maximum Games](https://en.gamegpu.com/view-user-profile?user=41287), 10 2025 June. Published in [Hardware](/backgammon/news/iron/) [...] ![rtx 5090 gigabyte](https://gamegpu.com/images/1_2025/NEWS/q2/f123/9600/rtx_5090_gigabyte.jpg)\\n\\n![rtx 5090 gigabyte](https://gamegpu.com/images/1_2025/NEWS/q2/f123/9600/rtx_5090_gigabyte.jpg) [...] Experts suggest that **NVIDIA's AIB Partners Will Hold Back RTX 5090 Stock**, waiting for the official return of tariffs. This will give the opportunity to raise prices after the deadline if agreements between the countries are not reached. A similar scheme was used in 2016-2017, when companies massively imported video cards before the start of trade restrictions. **RTX 5090 Availability May Worsen in July**, especially if tariffs are reintroduced and companies decide to pass the costs on to\", \"score\": 0.65173227, \"raw_content\": null}, {\"url\": \"https://www.notebookcheck.net/2025-Lenovo-Legion-Pro-7i-with-RTX-5090-and-Core-Ultra-9-275HX-gets-another-price-cut.1033540.0.html\", \"title\": \"2025 Lenovo Legion Pro 7i with RTX 5090 and Core Ultra 9 275HX ...\", \"content\": \"Deal | 2025 Lenovo Legion Pro 7i with RTX 5090 and Core Ultra 9 275HX gets another price cut ... It goes without saying that the top-end RTX 5090 configuration of\", \"score\": 0.64719695, \"raw_content\": null}, {\"url\": \"https://www.tomshardware.com/pc-components/gpus/cheapest-5090-ever-lucky-pc-builder-finds-rtx-5090-for-just-usd1-679-at-walmart-saving-usd1-520-card-has-worked-flawlessly-ever-since\", \"title\": \"Cheapest 5090 ever? Lucky PC builder finds RTX 5090 for 'just ...\", \"content\": \"Redditor Sufficient\\\\_Crazy7758 took to the platform to hail their purchase of the \\\"Cheapest 5090 ever?\\\", after scooping a PNY GeForce RTX 5090 from Walmart for the relatively low price of $1,679.99. The attached images reveal that the card was discounted by over $1,500 on May 25, having previously been discounted on May 13.\\n\\n\\\"Just so all of you know, I bought this on that date and it has worked flawlessly ever since,\\\" they said triumphantly. [...] In the ever-evolving world of insane GPU prices, tariffs, and frightful stock shortages, one intrepid PC builder has managed to swipe an Nvidia RTX 5090 from Walmart for \\\"just\\\" $1,679, more than $200 below MSRP. [...] even the [Founders Edition still retails for the same price](https://goto.walmart.com/c/1943169/565706/9383?subId1=tomshardware-us-9572975325724004555&sharedId=tomshardware-us&u=https%3A%2F%2Fwww.walmart.com%2Fip%2FNVIDIA-GeForce-RTX-5090-Graphic-Card-32GB-GDDR7%2F15176552726%3FclassType%3DREGULAR%26from%3D%2Fsearch), way over the $1,999 MSRP.\", \"score\": 0.6016175, \"raw_content\": null}], \"response_time\": 2.98}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(response['messages'][4].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c12011d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the price of 5090?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n"
     ]
    }
   ],
   "source": [
    "user_input = \"What is the price of 5090?\"\n",
    "\n",
    "for step in agent.stream(\n",
    "    {\"messages\": user_input},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cea72b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from backend.src.agent.tools_and_schemas import SearchQueryList, Reflection\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langgraph.types import Send\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph import START, END\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from google.genai import Client\n",
    "\n",
    "from backend.src.agent.state import (\n",
    "    OverallState,\n",
    "    QueryGenerationState,\n",
    "    ReflectionState,\n",
    "    WebSearchState,\n",
    ")\n",
    "from backend.src.agent.configuration import Configuration\n",
    "from backend.src.agent.prompts import (\n",
    "    get_current_date,\n",
    "    query_writer_instructions,\n",
    "    web_searcher_instructions,\n",
    "    reflection_instructions,\n",
    "    answer_instructions,\n",
    ")\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "from backend.src.agent.utils import (\n",
    "    get_citations,\n",
    "    get_research_topic,\n",
    "    insert_citation_markers,\n",
    "    resolve_urls,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997dcb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_query(state: OverallState, config: RunnableConfig) -> QueryGenerationState:\n",
    "    \"\"\"LangGraph node that generates a search queries based on the User's question.\n",
    "\n",
    "    Uses Gemini 2.0 Flash to create an optimized search query for web research based on\n",
    "    the User's question.\n",
    "\n",
    "    Args:\n",
    "        state: Current graph state containing the User's question\n",
    "        config: Configuration for the runnable, including LLM provider settings\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with state update, including search_query key containing the generated query\n",
    "    \"\"\"\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "\n",
    "    # check for custom initial search query count\n",
    "    if state.get(\"initial_search_query_count\") is None:\n",
    "        state[\"initial_search_query_count\"] = configurable.number_of_initial_queries\n",
    "\n",
    "    # init Gemini 2.0 Flash\n",
    "    # llm = ChatGoogleGenerativeAI(\n",
    "    #     model=configurable.query_generator_model,\n",
    "    #     temperature=1.0,\n",
    "    #     max_retries=2,\n",
    "    #     api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "    # )\n",
    "    llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=1,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    )\n",
    "    structured_llm = llm.with_structured_output(SearchQueryList)\n",
    "    print(get_research_topic(state[\"messages\"]),)\n",
    "    # Format the prompt\n",
    "    current_date = get_current_date()\n",
    "    formatted_prompt = query_writer_instructions.format(\n",
    "        current_date=current_date,\n",
    "        research_topic=get_research_topic(state[\"messages\"]),\n",
    "        number_queries=state[\"initial_search_query_count\"],\n",
    "    )\n",
    "    # Generate the search queries\n",
    "    result = structured_llm.invoke(formatted_prompt)\n",
    "    print(result)\n",
    "    return {\"query_list\": result.query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3125633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_to_web_research(state: QueryGenerationState):\n",
    "    \"\"\"LangGraph node that sends the search queries to the web research node.\n",
    "\n",
    "    This is used to spawn n number of web research nodes, one for each search query.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        Send(\"web_research\", {\"search_query\": search_query, \"id\": int(idx)})\n",
    "        for idx, search_query in enumerate(state[\"query_list\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "def web_research(state: WebSearchState, config: RunnableConfig) -> OverallState:\n",
    "    \"\"\"LangGraph node that performs web research using the native Google Search API tool.\n",
    "\n",
    "    Executes a web search using the native Google Search API tool in combination with Gemini 2.0 Flash.\n",
    "\n",
    "    Args:\n",
    "        state: Current graph state containing the search query and research loop count\n",
    "        config: Configura\"tion for the runnable, including search API settings\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with state update, including sources_gathered, research_loop_count, and web_research_results\n",
    "    \"\"\"\n",
    "    # Configure\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    formatted_prompt = web_searcher_instructions.format(\n",
    "        current_date=get_current_date(),\n",
    "        research_topic=state[\"search_query\"],\n",
    "    )\n",
    "    llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=1,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    )\n",
    "\n",
    "    # Create the agent with the formatted prompt as system message\n",
    "    agent = create_react_agent(\n",
    "        llm,\n",
    "        [tavily_search_tool],\n",
    "        prompt=formatted_prompt\n",
    "    )\n",
    "    # Uses the google genai client as the langchain client doesn't return grounding metadata\n",
    "    response = genai_client.models.generate_content(\n",
    "        model=configurable.query_generator_model,\n",
    "        contents=formatted_prompt,\n",
    "        config={\n",
    "            \"tools\": [{\"google_search\": {}}],\n",
    "            \"temperature\": 0,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    \n",
    "    # resolve the urls to short urls for saving tokens and time\n",
    "    resolved_urls = resolve_urls(\n",
    "        response.candidates[0].grounding_metadata.grounding_chunks, state[\"id\"]\n",
    "    )\n",
    "    # Gets the citations and adds them to the generated text\n",
    "    citations = get_citations(response, resolved_urls)\n",
    "    modified_text = insert_citation_markers(response.text, citations)\n",
    "    sources_gathered = [item for citation in citations for item in citation[\"segments\"]]\n",
    "\n",
    "    return {\n",
    "        \"sources_gathered\": sources_gathered,\n",
    "        \"search_query\": [state[\"search_query\"]],\n",
    "        \"web_research_result\": [modified_text],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f97d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3888798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(OverallState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "builder.add_node(\"generate_query\", generate_query)\n",
    "builder.add_node(\"web_research\", web_research)\n",
    "\n",
    "builder.add_edge(START, \"generate_query\")\n",
    "builder.add_conditional_edges(\n",
    "    \"generate_query\", continue_to_web_research, [\"web_research\"]\n",
    ")\n",
    "builder.add_edge(\"web_research\", END)\n",
    "\n",
    "graph = builder.compile(name=\"pro-search-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed0c4611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict the tendency of bitcoin in the next 6 months\n"
     ]
    }
   ],
   "source": [
    "# graph.invoke('predict the tendency of bitcoin in the next 6 months')\n",
    "state = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"predict the tendency of bitcoin in the next 6 months\"}], \"max_research_loops\": 3, \"initial_search_query_count\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "408ee220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Who won the euro 2024', additional_kwargs={}, response_metadata={}, id='fd8d9958-b982-47af-9603-ee168eec1929')],\n",
       " 'search_query': [],\n",
       " 'web_research_result': [],\n",
       " 'sources_gathered': [],\n",
       " 'initial_search_query_count': 3,\n",
       " 'max_research_loops': 3}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea88855f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef21ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(OverallState, config_schema=Configuration)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "builder.add_node(\"generate_query\", generate_query)\n",
    "builder.add_node(\"web_research\", web_research)\n",
    "builder.add_edge(START, \"generate_query\")\n",
    "builder.add_conditional_edges(\n",
    "    \"generate_query\", continue_to_web_research, [\"web_research\"]\n",
    ")\n",
    "\n",
    "builder.add_edge(\"finalize_answer\", END)\n",
    "\n",
    "graph = builder.compile(name=\"pro-search-agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1fc64",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'ModelMetaclass' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m formatted_prompt \u001b[38;5;241m=\u001b[39m web_searcher_instructions\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     16\u001b[0m     current_date\u001b[38;5;241m=\u001b[39mget_current_date(),\n\u001b[1;32m     17\u001b[0m     research_topic\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice of 5090\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m messages \u001b[38;5;241m=\u001b[39m [HumanMessage(content\u001b[38;5;241m=\u001b[39mformatted_prompt)]\n\u001b[0;32m---> 21\u001b[0m query_generation_response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m     22\u001b[0m     messages,\n\u001b[1;32m     23\u001b[0m     tools\u001b[38;5;241m=\u001b[39m[LLMGeneratedSearchQueries]\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m generated_queries \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m query_generation_response\u001b[38;5;241m.\u001b[39mtool_calls:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:372\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    368\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    369\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 372\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    373\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    374\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    375\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    376\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    377\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    378\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    379\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    380\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    381\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    382\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:957\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    955\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    956\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:776\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    775\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 776\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    777\u001b[0m                 m,\n\u001b[1;32m    778\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    779\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    780\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    781\u001b[0m             )\n\u001b[1;32m    782\u001b[0m         )\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1022\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m   1023\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1026\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:891\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     stream_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[1;32m    888\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    889\u001b[0m     )\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[0;32m--> 891\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_request_payload(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    892\u001b[0m generation_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m payload:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:946\u001b[0m, in \u001b[0;36mBaseChatOpenAI._get_request_payload\u001b[0;34m(self, input_, stop, **kwargs)\u001b[0m\n\u001b[1;32m    943\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n\u001b[1;32m    945\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_responses_api(payload):\n\u001b[1;32m    947\u001b[0m     payload \u001b[38;5;241m=\u001b[39m _construct_responses_api_payload(messages, payload)\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:932\u001b[0m, in \u001b[0;36mBaseChatOpenAI._use_responses_api\u001b[0;34m(self, payload)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_responses_api\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _use_responses_api(payload)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:2866\u001b[0m, in \u001b[0;36m_use_responses_api\u001b[0;34m(payload)\u001b[0m\n\u001b[1;32m   2865\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_use_responses_api\u001b[39m(payload: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m-> 2866\u001b[0m     uses_builtin_tools \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m payload \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   2867\u001b[0m         _is_builtin_tool(tool) \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2868\u001b[0m     )\n\u001b[1;32m   2869\u001b[0m     responses_only_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_response_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(uses_builtin_tools \u001b[38;5;129;01mor\u001b[39;00m responses_only_args\u001b[38;5;241m.\u001b[39mintersection(payload))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:2867\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2865\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_use_responses_api\u001b[39m(payload: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   2866\u001b[0m     uses_builtin_tools \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m payload \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m-> 2867\u001b[0m         _is_builtin_tool(tool) \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m payload[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2868\u001b[0m     )\n\u001b[1;32m   2869\u001b[0m     responses_only_args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_response_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(uses_builtin_tools \u001b[38;5;129;01mor\u001b[39;00m responses_only_args\u001b[38;5;241m.\u001b[39mintersection(payload))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:2862\u001b[0m, in \u001b[0;36m_is_builtin_tool\u001b[0;34m(tool)\u001b[0m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_builtin_tool\u001b[39m(tool: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m-> 2862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tool \u001b[38;5;129;01mand\u001b[39;00m tool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'ModelMetaclass' is not iterable"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "class LLMGeneratedSearchQueries(BaseModel):\n",
    "    \"\"\"Schema for the LLM to generate a list of search queries.\"\"\"\n",
    "    search_queries: list[str] = Field(description=\"A list of search queries to find relevant information based on the research topic.\")\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=1024,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "    )\n",
    "\n",
    "messages = [HumanMessage(content=formatted_prompt)]\n",
    "\n",
    "query_generation_response = llm.invoke(\n",
    "    messages,\n",
    "    tools=[LLMGeneratedSearchQueries]\n",
    ")\n",
    "\n",
    "generated_queries = []\n",
    "if query_generation_response.tool_calls:\n",
    "    for tool_call in query_generation_response.tool_calls:\n",
    "        if tool_call['name'] == LLMGeneratedSearchQueries.__name__:\n",
    "            generated_queries.extend(tool_call['args'].get('search_queries', []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34382436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
